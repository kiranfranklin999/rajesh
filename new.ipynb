{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import random,os,io\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint,CSVLogger\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42039, 16), (42039, 1), (10935, 16), (10935, 1), (10934, 16), (10934, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dep_train=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XTr.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_train=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yTr.dat\",sep='\\s+',names=['target'])\n",
    "dep_val=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XV.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_val=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yV.dat\",sep='\\s+',names=[\"target\"])\n",
    "dep_test=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XT.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_test=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yT.dat\",sep='\\s+',names=[\"target\"])\n",
    "\n",
    "dep_train.shape,indep_train.shape,dep_val.shape,indep_val.shape,dep_test.shape,indep_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "batch_size = 50\n",
    "num_hidden_1 = 12\n",
    "num_hidden_2 = 12\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 100\n",
    "dropout_keep_prob = 0.3 # set to no dropout by default\n",
    "\n",
    "# variable to control the resolution at which the training results are stored\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = tf.keras.Sequential()\n",
    "reg.add(Dense(units = 6, activation = 'relu', input_dim = 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.add(Dense(units = 6, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.add(Dense(units = 6, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.add(Dense(units = 1, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_17 (Dense)            (None, 6)                 102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102\n",
      "Trainable params: 102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "reg.compile(optimizer=optimizer,  \n",
    "            loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4204/4204 [==============================] - 6s 1ms/step - loss: 0.0043\n",
      "Epoch 2/10\n",
      "4204/4204 [==============================] - 8s 2ms/step - loss: 9.9983e-04\n",
      "Epoch 3/10\n",
      "4204/4204 [==============================] - 12s 3ms/step - loss: 8.0338e-04\n",
      "Epoch 4/10\n",
      "4204/4204 [==============================] - 13s 3ms/step - loss: 6.5178e-04\n",
      "Epoch 5/10\n",
      "4204/4204 [==============================] - 13s 3ms/step - loss: 5.4190e-04\n",
      "Epoch 6/10\n",
      "4204/4204 [==============================] - 13s 3ms/step - loss: 5.0964e-04\n",
      "Epoch 7/10\n",
      "4204/4204 [==============================] - 12s 3ms/step - loss: 4.8497e-04\n",
      "Epoch 8/10\n",
      "4204/4204 [==============================] - 10s 2ms/step - loss: 4.5276e-04\n",
      "Epoch 9/10\n",
      "4204/4204 [==============================] - 11s 3ms/step - loss: 4.5096e-04\n",
      "Epoch 10/10\n",
      "4204/4204 [==============================] - 12s 3ms/step - loss: 4.2349e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19d5a80d700>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(dep_train, indep_train, batch_size = 10, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: demo\\assets\n"
     ]
    }
   ],
   "source": [
    "reg.save(\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 6)                 102       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reg_reload=tf.keras.models.load_model(\"demo\")\n",
    "reg_reload.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 2s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.01661042],\n",
       "       [0.01399928],\n",
       "       [0.01650758],\n",
       "       ...,\n",
       "       [0.01186464],\n",
       "       [0.01621098],\n",
       "       [0.00988809]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_reload.predict(dep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class regclass(Model):\n",
    "    def __init__(self):\n",
    "        super(regclass,self).__init__()\n",
    "        self.W1 = self.add_weight(shape = (16,6), initializer = 'random_normal', trainable = True)\n",
    "        self.b1 = self.add_weight(shape = (6,), initializer = 'random_normal', trainable = True)\n",
    "        self.dp1 = tf.keras.layers.Dropout(0.2)\n",
    "        self.W2 = self.add_weight(shape = (6,6), initializer = 'random_normal', trainable = True)\n",
    "        self.b2 = self.add_weight(shape = (6,), initializer = 'random_normal', trainable = True)\n",
    "        self.dp2 = tf.keras.layers.Dropout(0.2)\n",
    "        self.W3 = self.add_weight(shape = (6,1), initializer = 'random_normal', trainable = True)\n",
    "        self.b3 = self.add_weight(shape = (1,), initializer = 'random_normal', trainable = True)\n",
    "    \n",
    "\n",
    "    def call(self,x):\n",
    "        out = x\n",
    "\n",
    "        out = tf.nn.relu(tf.matmul(out, self.W1) + self.b1)\n",
    "        out = self.dp1(out)\n",
    "        out = tf.nn.relu(tf.matmul(out, self.W2) + self.b2)\n",
    "        out = self.dp2(out)\n",
    "        out = tf.matmul(out, self.W3) + self.b3\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cus(Model):\n",
    "    def __init__(self) -> None:\n",
    "        super(cus,self).__init__()\n",
    "        self.dense1= Dense(units = 6, activation = 'relu', input_dim = 16)\n",
    "        self.dense2= Dense(units = 6, activation = 'relu')\n",
    "        self.dense3= Dense(units = 6, activation = 'relu')\n",
    "        self.dense4= Dense(units = 1, activation = 'linear')\n",
    "\n",
    "    def call(self,x):\n",
    "        out = self.dense1(x)\n",
    "        out = self.dense2(out)\n",
    "        out = self.dense3(out)\n",
    "        out = self.dense4(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_cus= regclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_cus.compile(optimizer=optimizer,  \n",
    "            loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1314/1314 [==============================] - 14s 9ms/step - loss: 0.0012\n",
      "Epoch 2/2\n",
      "1314/1314 [==============================] - 5s 4ms/step - loss: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19d59836d90>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_cus.fit(dep_train, indep_train, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"regclass_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dropout_1 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reg_cus.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SobolevNetwork(Model):\n",
    "    def __init__(self, input_dim, num_hidden,init = None):\n",
    "        super(SobolevNetwork, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.num_hidden = num_hidden\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.input_dim, self.num_hidden],stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.W4 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b4 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.W5 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b5 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.W6 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b6 = tf.Variable(tf.ones([self.num_hidden]))        \n",
    "        self.W7 = tf.Variable(tf.random.normal([self.num_hidden, 1],stddev=0.1))\n",
    "        self.b7 = tf.Variable(tf.ones([1]))\n",
    "        self.w = [(self.W1, self.b1), (self.W2, self.b2), (self.W3, self.b3),(self.W4, self.b4), (self.W5, self.b5), (self.W6, self.b6),(self.W7, self.b7)]\n",
    "        \n",
    "    def call(self, X):\n",
    "        #Input layer\n",
    "        out = X\n",
    "        #Hidden layers\n",
    "        W,b = self.w[0]\n",
    "        out = tf.nn.tanh(tf.matmul(out, W) + b)\n",
    "        W,b = self.w[1]\n",
    "        out = tf.nn.tanh(tf.matmul(out, W) + b)\n",
    "        W,b = self.w[2]\n",
    "        out = tf.nn.sigmoid(tf.matmul(out, W) + b)\n",
    "        W,b = self.w[3]\n",
    "        out = tf.nn.sigmoid(tf.matmul(out, W) + b)\n",
    "        W,b = self.w[4]\n",
    "        out = tf.nn.leaky_relu(tf.matmul(out, W) + b)\n",
    "        W,b = self.w[5]\n",
    "        out = tf.nn.relu(tf.matmul(out, W) + b)\n",
    "        #Output layer\n",
    "        W,b = self.w[-1]\n",
    "        out = tf.matmul(out, W) + b\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\n For Epoch Number {} the model has loss[mean_absolute_error] of {}\".format(epoch+1, logs[\"loss\"]))\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        print(\"\\n For Batch Number {} the model has loss[mean_absolute_error] of {}\".format(batch+1, logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_callback = CSVLogger( 'logs.csv',separator=',',append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping( monitor='val_loss',min_delta=0,patience=2, verbose=1, mode=\"auto\", baseline=None, restore_best_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedular(epoch, lr):\n",
    "    if epoch <= 3:\n",
    "        learning_rate = lr\n",
    "    else:\n",
    "        learning_rate = lr * tf.math.exp(-0.1)\n",
    "        learning_rate = learning_rate.numpy()\n",
    "    \n",
    "\n",
    "    # with train_writer.as_default():\n",
    "    #     tf.summary.scalar('LearningRateScheduler', data= learning_rate, step= epoch)\n",
    "    \n",
    "    return learning_rate\n",
    "\n",
    "schedular_callback = LearningRateScheduler(schedular, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_snn= SobolevNetwork(input_dim=16,num_hidden=num_hidden_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_snn.compile(optimizer=optimizer,  \n",
    "            loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0011676228605210781}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0012\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0011432855390012264}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.0010535717010498047}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.0011435628402978182}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0012263270327821374}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0012\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0011148732155561447}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.001069816411472857}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0010683939326554537}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0010464676888659596}\n",
      " 9/43 [=====>........................] - ETA: 0s - loss: 0.0010\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.0010854026768356562}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0010986995184794068}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0010970544535666704}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.0010947068221867085}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.0010784525657072663}\n",
      "14/43 [========>.....................] - ETA: 0s - loss: 0.0011\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.001049640355631709}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.001077396096661687}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.001094773062504828}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0010761972516775131}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0010588428704068065}\n",
      "19/43 [============>.................] - ETA: 0s - loss: 0.0011\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.001072383252903819}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0010805620113387704}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0010746391490101814}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0010710068745538592}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0010771321831271052}\n",
      "24/43 [===============>..............] - ETA: 0s - loss: 0.0011\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0010807026410475373}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0010710915084928274}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.0010606036521494389}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0010719294659793377}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.001077780849300325}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0010679056867957115}\n",
      "30/43 [===================>..........] - ETA: 0s - loss: 0.0011\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.0010571375023573637}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.0010696682147681713}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0010786887723952532}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0010720088612288237}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.001061363727785647}\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0011\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.0010719187557697296}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0010800972813740373}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.0010765219340100884}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.001069786841981113}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0010791161330416799}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0010847746161743999}\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0011\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0010756084229797125}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.0010753890965133905}\n",
      "\n",
      " For Epoch Number 1 the model has loss[mean_absolute_error] of 0.0010753890965133905\n",
      "43/43 [==============================] - 4s 101ms/step - loss: 0.0011 - val_loss: 0.0013 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0013416847214102745}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0013\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0011338943149894476}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.0011893855407834053}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.0012167661916464567}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0011617755517363548}\n",
      "\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0011510970070958138}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0011362896766513586}\n",
      " 7/43 [===>..........................] - ETA: 0s - loss: 0.0011\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0010749595239758492}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.00112793012522161}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.001170580624602735}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0011138493428006768}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0010929903946816921}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.0010890699923038483}\n",
      "13/43 [========>.....................] - ETA: 0s - loss: 0.0011\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.001061179325915873}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.0010806455975398421}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0010776589624583721}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.001097379601560533}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0011235068086534739}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0010904672089964151}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.001093323458917439}\n",
      "20/43 [============>.................] - ETA: 0s - loss: 0.0011\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0010650550248101354}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0010659509571269155}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0010432535782456398}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0010388121008872986}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0010282628936693072}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0010093242162838578}\n",
      "26/43 [=================>............] - ETA: 0s - loss: 0.0010\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.0010144618572667241}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0009975568391382694}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.0010030907578766346}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0009921507444232702}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.0010134782642126083}\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 0.0010\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.001029924605973065}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0010203674901276827}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0010182972764596343}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.0010147090069949627}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.0010028546676039696}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0010060432832688093}\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0010\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.000991982058621943}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.001000674208626151}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0009970160899683833}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0010131163289770484}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0010344319744035602}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.0010339259169995785}\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0010\n",
      " For Epoch Number 2 the model has loss[mean_absolute_error] of 0.0010339259169995785\n",
      "43/43 [==============================] - 5s 130ms/step - loss: 0.0010 - val_loss: 0.0025 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0025007862132042646}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0027652049902826548}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.002432584762573242}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.002223934978246689}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0022753262892365456}\n",
      "\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0021692286245524883}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0020384960807859898}\n",
      " 7/43 [===>..........................] - ETA: 0s - loss: 0.0020\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.002015941310673952}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0018588145030662417}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.0018892544321715832}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.001927751349285245}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0018537365831434727}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.0018996460130438209}\n",
      "13/43 [========>.....................] - ETA: 0s - loss: 0.0019\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.001993639161810279}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.0020036508794873953}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0019294550875201821}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.001912033068947494}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0018463217420503497}\n",
      "18/43 [===========>..................] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0018512250389903784}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.0018684413516893983}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0018175123259425163}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.001846135943196714}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0019117506453767419}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0019343956373631954}\n",
      "24/43 [===============>..............] - ETA: 0s - loss: 0.0019\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0018777286168187857}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0018664926756173372}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.0018383688293397427}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0018306715646758676}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.0018358113011345267}\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0017963879508897662}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.0018142471089959145}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.001843301928602159}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0018308304715901613}\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0018333527259528637}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.0018592169508337975}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.001862216740846634}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0018394228536635637}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.0018310585292056203}\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.001794756157323718}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0017800107598304749}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0017524812137708068}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0017475778004154563}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.0017469731392338872}\n",
      "\n",
      " For Epoch Number 3 the model has loss[mean_absolute_error] of 0.0017469731392338872\n",
      "43/43 [==============================] - 4s 97ms/step - loss: 0.0017 - val_loss: 0.0012 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 4/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0012635282473638654}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0013\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0013137395726516843}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.001107406453229487}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.000986009370535612}\n",
      " 4/43 [=>............................] - ETA: 0s - loss: 9.8601e-04\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.001026232959702611}\n",
      "\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0010024536168202758}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0010560124646872282}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0010968944989144802}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0010615444043651223}\n",
      " 9/43 [=====>........................] - ETA: 0s - loss: 0.0011    \n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.0010407548397779465}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.001030268962495029}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.001002540928311646}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.0010547046549618244}\n",
      "13/43 [========>.....................] - ETA: 0s - loss: 0.0011\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.0010960969375446439}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.00105996651109308}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0010673128999769688}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.001037313835695386}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0010390771785750985}\n",
      "18/43 [===========>..................] - ETA: 0s - loss: 0.0010\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0010146544082090259}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.001029155682772398}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0010179830715060234}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0010523127857595682}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0010946033289656043}\n",
      "23/43 [===============>..............] - ETA: 0s - loss: 0.0011\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.001076448243111372}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.001128059346228838}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0011978474212810397}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.001211275695823133}\n",
      "27/43 [=================>............] - ETA: 0s - loss: 0.0012\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0012341560795903206}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.0012892395025119185}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0013131361920386553}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.001292557455599308}\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 0.0013\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.0012930198572576046}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0012709047878161073}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0012572996784001589}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.0012391296913847327}\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0012\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.0012172858696430922}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0011985884048044682}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.0011898683151230216}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.0011741442140191793}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0011586365289986134}\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0012\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0011465350398793817}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.00114039471372962}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.0011396693298593163}\n",
      "\n",
      " For Epoch Number 4 the model has loss[mean_absolute_error] of 0.0011396693298593163\n",
      "43/43 [==============================] - 5s 124ms/step - loss: 0.0011 - val_loss: 0.0014 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0009048374486155808.\n",
      "Epoch 5/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0013155991910025477}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0013\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0010292327497154474}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.0012150865513831377}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.0013643992133438587}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0012233162997290492}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0012\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0013805466005578637}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0015720926458016038}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0015829462790861726}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0015611046692356467}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.0016394408885389566}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0016287276521325111}\n",
      "11/43 [======>.......................] - ETA: 0s - loss: 0.0016\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0015904551837593317}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.0015908072236925364}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.0015195899177342653}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.0015129642561078072}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0014844953548163176}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.0014595362590625882}\n",
      "17/43 [==========>...................] - ETA: 0s - loss: 0.0015\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0014410570729523897}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0014074654318392277}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.0013763518072664738}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0013567284913733602}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.001322123920544982}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0013284473679959774}\n",
      "23/43 [===============>..............] - ETA: 0s - loss: 0.0013\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0013305441243574023}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0012986413203179836}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0012869086349382997}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.00126742257270962}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0012443782761693}\n",
      "28/43 [==================>...........] - ETA: 0s - loss: 0.0012\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.0012393739307299256}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.001222020946443081}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.00123094511218369}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.0012366874143481255}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0012170401168987155}\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0012\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0012177309254184365}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.0011976088862866163}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.001203404157422483}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0012013110099360347}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.0011956306407228112}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.0011905785650014877}\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0012\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0011863247491419315}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.001178803271614015}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0011735042789950967}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.001172965974546969}\n",
      "\n",
      " For Epoch Number 5 the model has loss[mean_absolute_error] of 0.001172965974546969\n",
      "43/43 [==============================] - 5s 121ms/step - loss: 0.0012 - val_loss: 0.0014 - lr: 9.0484e-04\n",
      "Epoch 5: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19d5fb99640>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_snn.fit(dep_train, indep_train, validation_data=(dep_val,indep_val),batch_size = 1000, epochs = 8, callbacks=[schedular_callback, es_callback,LossCallback(), csv_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sobolev_network_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      "=================================================================\n",
      "Total params: 997\n",
      "Trainable params: 997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "custom_snn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(custom_snn, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphvizNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 2s 6ms/step - loss: 0.0013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0013376888819038868"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_snn.evaluate(dep_test, indep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13bec6b3e23dd0921d86a1ec88a5a342f8b423eb92f661fc2c47e8b6a92d2fe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
