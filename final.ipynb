{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import random,os,io\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint,CSVLogger\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42039, 16), (42039, 1), (10935, 16), (10935, 1), (10934, 16), (10934, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dep_train=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XTr.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_train=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yTr.dat\",sep='\\s+',names=['target'])\n",
    "dep_val=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XV.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_val=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yV.dat\",sep='\\s+',names=[\"target\"])\n",
    "dep_test=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XT.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_test=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yT.dat\",sep='\\s+',names=[\"target\"])\n",
    "\n",
    "dep_train.shape,indep_train.shape,dep_val.shape,indep_val.shape,dep_test.shape,indep_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SobolevNetwork(Model):\n",
    "    def __init__(self, input_dim, num_hidden,init = None):\n",
    "        super(SobolevNetwork, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.num_hidden = num_hidden\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.input_dim, self.num_hidden],stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.dp1 = tf.keras.layers.Dropout(0.2)\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.dp2 = tf.keras.layers.Dropout(0.2)\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.dp3 = tf.keras.layers.Dropout(0.2)\n",
    "        self.W4 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b4 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.dp4 = tf.keras.layers.Dropout(0.2)\n",
    "        self.W5 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b5 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.dp5 = tf.keras.layers.Dropout(0.2)\n",
    "        self.W6 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b6 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.dp6 = tf.keras.layers.Dropout(0.2)        \n",
    "        self.W7 = tf.Variable(tf.random.normal([self.num_hidden, 1],stddev=0.1))\n",
    "        self.b7 = tf.Variable(tf.ones([1]))\n",
    "        self.w = [(self.W1, self.b1), (self.W2, self.b2), (self.W3, self.b3),(self.W4, self.b4), (self.W5, self.b5), (self.W6, self.b6),(self.W7, self.b7)]\n",
    "        \n",
    "    def call(self, X):\n",
    "        #Input layer\n",
    "        out = X\n",
    "        #Hidden layers\n",
    "        W,b = self.w[0]\n",
    "        out = tf.nn.tanh(tf.matmul(out, W) + b)\n",
    "        out = self.dp1(out)\n",
    "        W,b = self.w[1]\n",
    "        out = tf.nn.tanh(tf.matmul(out, W) + b)\n",
    "        out = self.dp2(out)\n",
    "        W,b = self.w[2]\n",
    "        out = tf.nn.sigmoid(tf.matmul(out, W) + b)\n",
    "        out = self.dp3(out)\n",
    "        W,b = self.w[3]\n",
    "        out = tf.nn.sigmoid(tf.matmul(out, W) + b)\n",
    "        out = self.dp4(out)\n",
    "        W,b = self.w[4]\n",
    "        out = tf.nn.leaky_relu(tf.matmul(out, W) + b)\n",
    "        out = self.dp5(out)\n",
    "        W,b = self.w[5]\n",
    "        out = tf.nn.relu(tf.matmul(out, W) + b)\n",
    "        #Output layer\n",
    "        W,b = self.w[-1]\n",
    "        out = tf.matmul(out, W) + b\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\n For Epoch Number {} the model has loss[mean_absolute_error] of {}\".format(epoch+1, logs[\"loss\"]))\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        print(\"\\n For Batch Number {} the model has loss[mean_absolute_error] of {}\".format(batch+1, logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_callback = CSVLogger( 'logs.csv',separator=',',append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping( monitor='val_loss',min_delta=0,patience=2, verbose=1, mode=\"auto\", baseline=None, restore_best_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedular(epoch, lr):\n",
    "    if epoch <= 3:\n",
    "        learning_rate = lr\n",
    "    else:\n",
    "        learning_rate = lr * tf.math.exp(-0.1)\n",
    "        learning_rate = learning_rate.numpy()\n",
    "    \n",
    "\n",
    "    # with train_writer.as_default():\n",
    "    #     tf.summary.scalar('LearningRateScheduler', data= learning_rate, step= epoch)\n",
    "    \n",
    "    return learning_rate\n",
    "\n",
    "schedular_callback = LearningRateScheduler(schedular, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_1 = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_snn= SobolevNetwork(input_dim=16,num_hidden=num_hidden_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_snn.compile(optimizer=tf.keras.optimizers.Adam(),  \n",
    "            loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 1.289264440536499}\n",
      " 1/43 [..............................] - ETA: 4:07 - loss: 1.2893\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 1.275917887687683}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 1.2621989250183105}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 1.2480359077453613}\n",
      " 4/43 [=>............................] - ETA: 0s - loss: 1.2480  \n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 1.2344616651535034}\n",
      "\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 1.221309781074524}\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0134s vs `on_train_batch_end` time: 0.0808s). Check your callbacks.\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 1.2079960107803345}\n",
      " 7/43 [===>..........................] - ETA: 0s - loss: 1.2080\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 1.194827675819397}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 1.1815218925476074}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 1.1681941747665405}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 1.1551072597503662}\n",
      "11/43 [======>.......................] - ETA: 0s - loss: 1.1551\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 1.1418266296386719}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 1.1287401914596558}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 1.1155614852905273}\n",
      "14/43 [========>.....................] - ETA: 0s - loss: 1.1156\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 1.1024469137191772}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 1.0894439220428467}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 1.0762923955917358}\n",
      "17/43 [==========>...................] - ETA: 0s - loss: 1.0763\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 1.0631589889526367}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 1.049976110458374}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 1.0367810726165771}\n",
      "20/43 [============>.................] - ETA: 0s - loss: 1.0368\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 1.0236748456954956}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 1.0105637311935425}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.9973464608192444}\n",
      "23/43 [===============>..............] - ETA: 0s - loss: 0.9973\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.9841606616973877}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.9708545804023743}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.9575810432434082}\n",
      "26/43 [=================>............] - ETA: 0s - loss: 0.9576\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.9443992972373962}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.9312126636505127}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.917996883392334}\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 0.9180\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.9046636819839478}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.8912336230278015}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.8778892755508423}\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.8779\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.8645815849304199}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.8511107563972473}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.8375280499458313}\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.8375\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.8239910006523132}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.8101939558982849}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.7966037392616272}\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.7966\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.7828266024589539}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.7690930366516113}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.7553134560585022}\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.7553\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.7414572238922119}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.7408958673477173}\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.7409\n",
      " For Epoch Number 1 the model has loss[mean_absolute_error] of 0.7408958673477173\n",
      "43/43 [==============================] - 8s 45ms/step - loss: 0.7409 - val_loss: 0.0960 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.12368489056825638}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.1237\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.11822246760129929}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.11326595395803452}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.10843200236558914}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.10613784193992615}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.1061\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.1055414006114006}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.10575395077466965}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.10726354271173477}\n",
      " 8/43 [====>.........................] - ETA: 0s - loss: 0.1073\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.10830322653055191}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.10975091904401779}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.11097273230552673}\n",
      "11/43 [======>.......................] - ETA: 0s - loss: 0.1110\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.11232167482376099}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.11334162205457687}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.11423114687204361}\n",
      "14/43 [========>.....................] - ETA: 0s - loss: 0.1142\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.11494951695203781}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.11525817215442657}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.11505123227834702}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.11451307684183121}\n",
      "18/43 [===========>..................] - ETA: 0s - loss: 0.1145\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.11383535712957382}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.1131991297006607}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.11242837458848953}\n",
      "21/43 [=============>................] - ETA: 0s - loss: 0.1124\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.11151614785194397}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.1105751022696495}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.10970684885978699}\n",
      "24/43 [===============>..............] - ETA: 0s - loss: 0.1097\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.1087418720126152}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.10793475806713104}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.10711852461099625}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.10655024647712708}\n",
      "28/43 [==================>...........] - ETA: 0s - loss: 0.1066\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.10620259493589401}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.10562245547771454}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.10507343709468842}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.10457006096839905}\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.1046\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.1041080504655838}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.10358873009681702}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.1030307337641716}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.10259523242712021}\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.1026\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.10224248468875885}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.1017133817076683}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.10131699591875076}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.10082985460758209}\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.1008\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.10036589950323105}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.09990951418876648}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.09989184886217117}\n",
      "\n",
      " For Epoch Number 2 the model has loss[mean_absolute_error] of 0.09989184886217117\n",
      "43/43 [==============================] - 1s 22ms/step - loss: 0.0999 - val_loss: 0.0022 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.07965748757123947}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0797\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0810239240527153}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.08070850372314453}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.08129692077636719}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.08153486251831055}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0815\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.08115677535533905}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.08102212101221085}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.08118076622486115}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.08091119676828384}\n",
      " 9/43 [=====>........................] - ETA: 0s - loss: 0.0809\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.08090756833553314}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0809764713048935}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.08090611547231674}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.08065672963857651}\n",
      "13/43 [========>.....................] - ETA: 0s - loss: 0.0807\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.0805073231458664}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.08020028471946716}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.07986057549715042}\n",
      "16/43 [==========>...................] - ETA: 0s - loss: 0.0799\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.07942184060811996}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.07930304110050201}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.07896500080823898}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.07877996563911438}\n",
      "20/43 [============>.................] - ETA: 0s - loss: 0.0788\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.07851813733577728}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.07829892635345459}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.07814859598875046}\n",
      "23/43 [===============>..............] - ETA: 0s - loss: 0.0781\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.07787254452705383}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0776485875248909}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0775110274553299}\n",
      "26/43 [=================>............] - ETA: 0s - loss: 0.0775\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.07745835930109024}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.07716421782970428}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.0770515650510788}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.07689052820205688}\n",
      "30/43 [===================>..........] - ETA: 0s - loss: 0.0769\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.07672957330942154}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.07651273161172867}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.07635808736085892}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.07615165412425995}\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0762\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.07597177475690842}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.07576578110456467}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.07560594379901886}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.07556542009115219}\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0756\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.07536126673221588}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.07522473484277725}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.07504349201917648}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.07484448701143265}\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0748\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.07485095411539078}\n",
      "\n",
      " For Epoch Number 3 the model has loss[mean_absolute_error] of 0.07485095411539078\n",
      "43/43 [==============================] - 1s 19ms/step - loss: 0.0749 - val_loss: 0.0034 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 4/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.06361724436283112}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0636\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.06540572643280029}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.06524085253477097}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.06551295518875122}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.06541165709495544}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0654\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.06532351672649384}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.06554532051086426}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.06487664580345154}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0651017352938652}\n",
      " 9/43 [=====>........................] - ETA: 0s - loss: 0.0651\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.06492342799901962}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.06480943411588669}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.06475017964839935}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.06493443250656128}\n",
      "13/43 [========>.....................] - ETA: 0s - loss: 0.0649\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.06503502279520035}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.06481131911277771}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.06467714160680771}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.0645262822508812}\n",
      "17/43 [==========>...................] - ETA: 0s - loss: 0.0645\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.06427016109228134}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.06417065113782883}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.06386218965053558}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.06361381709575653}\n",
      "21/43 [=============>................] - ETA: 0s - loss: 0.0636\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.06333786994218826}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.06324470788240433}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.06300920993089676}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.06277427822351456}\n",
      "25/43 [================>.............] - ETA: 0s - loss: 0.0628\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.06252610683441162}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.06233267858624458}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.06218668818473816}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.06206752732396126}\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 0.0621\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0619136281311512}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.06173906475305557}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.061454322189092636}\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0615\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0613081157207489}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.061192382127046585}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.06107797101140022}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.06085843965411186}\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0609\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.06071556359529495}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.060524605214595795}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.06031731516122818}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.06017599627375603}\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0602\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.059996575117111206}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.05978969484567642}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.05979796126484871}\n",
      "\n",
      " For Epoch Number 4 the model has loss[mean_absolute_error] of 0.05979796126484871\n",
      "43/43 [==============================] - 1s 20ms/step - loss: 0.0598 - val_loss: 0.0049 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0009048374486155808.\n",
      "Epoch 5/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0536535419523716}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0537\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0531596876680851}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.05220587179064751}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.05186397209763527}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.05193459615111351}\n",
      "\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.05174386128783226}\n",
      " 6/43 [===>..........................] - ETA: 0s - loss: 0.0517\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.05167483165860176}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0517875961959362}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.05159062519669533}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.05155353248119354}\n",
      "10/43 [=====>........................] - ETA: 0s - loss: 0.0516\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.05112779140472412}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.051013488322496414}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.05084207281470299}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.050554368644952774}\n",
      "14/43 [========>.....................] - ETA: 0s - loss: 0.0506\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.050244688987731934}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.050035085529088974}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.049913257360458374}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.04975828900933266}\n",
      "18/43 [===========>..................] - ETA: 0s - loss: 0.0498\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0495121031999588}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.049306657165288925}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.04922300577163696}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.049011602997779846}\n",
      "22/43 [==============>...............] - ETA: 0s - loss: 0.0490\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.048920873552560806}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.04877881333231926}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.04856700077652931}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.048309873789548874}\n",
      "26/43 [=================>............] - ETA: 0s - loss: 0.0483\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.04820410534739494}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.048049844801425934}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.0478508286178112}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.04770100489258766}\n",
      "30/43 [===================>..........] - ETA: 0s - loss: 0.0477\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.04762249439954758}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.04744821786880493}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.04735288769006729}\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0474\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.04719041287899017}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.04707498475909233}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.04689977318048477}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.04671495407819748}\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0467\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.046532243490219116}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.04637837037444115}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.04622943326830864}\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0462\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.04604600742459297}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.04588950052857399}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.045891616493463516}\n",
      "\n",
      " For Epoch Number 5 the model has loss[mean_absolute_error] of 0.045891616493463516\n",
      "43/43 [==============================] - 1s 19ms/step - loss: 0.0459 - val_loss: 0.0029 - lr: 9.0484e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0008187307976186275.\n",
      "Epoch 6/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0388210155069828}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0388\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.038756195455789566}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.037956610321998596}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.03754574432969093}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.03736587241292}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0374\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.03733810782432556}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.037138715386390686}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.037157513201236725}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.036963991820812225}\n",
      " 9/43 [=====>........................] - ETA: 0s - loss: 0.0370\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.036719441413879395}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.03668800741434097}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.03665813058614731}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.03650559484958649}\n",
      "13/43 [========>.....................] - ETA: 0s - loss: 0.0365\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.03638588637113571}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.03629874810576439}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.036148276180028915}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.035975586622953415}\n",
      "17/43 [==========>...................] - ETA: 0s - loss: 0.0360\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.035845693200826645}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.035726480185985565}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.03558303043246269}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.03547833114862442}\n",
      "21/43 [=============>................] - ETA: 0s - loss: 0.0355\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.035291604697704315}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.035131197422742844}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0349506177008152}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0348348431289196}\n",
      "25/43 [================>.............] - ETA: 0s - loss: 0.0348\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.03475232049822807}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.034606412053108215}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0344846248626709}\n",
      "28/43 [==================>...........] - ETA: 0s - loss: 0.0345\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.034394171088933945}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.03427984192967415}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.03414282947778702}\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 0.0341\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.034020449966192245}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.033878225833177567}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0337626188993454}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.033623017370700836}\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0336\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.03347129747271538}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.03333934023976326}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.033179137855768204}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.033009376376867294}\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0330\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.032845381647348404}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.03271505981683731}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.032540421932935715}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.032532356679439545}\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0325\n",
      " For Epoch Number 6 the model has loss[mean_absolute_error] of 0.032532356679439545\n",
      "43/43 [==============================] - 1s 19ms/step - loss: 0.0325 - val_loss: 0.0035 - lr: 8.1873e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0007408182718791068.\n",
      "Epoch 7/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.02658100426197052}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0266\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.02618655189871788}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.02600187622010708}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.025935519486665726}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.02593366429209709}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0259\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.02580230124294758}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.02575415000319481}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.025626735761761665}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0253949835896492}\n",
      " 9/43 [=====>........................] - ETA: 0s - loss: 0.0254\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.02529185824096203}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.025068752467632294}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.024922877550125122}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.024762004613876343}\n",
      "13/43 [========>.....................] - ETA: 0s - loss: 0.0248\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.024640914052724838}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.02452995441854}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.024386407807469368}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.024305451661348343}\n",
      "17/43 [==========>...................] - ETA: 0s - loss: 0.0243\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.024199387058615685}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.024090763181447983}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.02395601011812687}\n",
      "20/43 [============>.................] - ETA: 0s - loss: 0.0240\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.023813623934984207}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.02367519587278366}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.02359512448310852}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.023458274081349373}\n",
      "24/43 [===============>..............] - ETA: 0s - loss: 0.0235\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.023349419236183167}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.023228421807289124}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.02310197427868843}\n",
      "27/43 [=================>............] - ETA: 0s - loss: 0.0231\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.023004362359642982}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.022860106080770493}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.022755395621061325}\n",
      "30/43 [===================>..........] - ETA: 0s - loss: 0.0228\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.02264505811035633}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.022535989060997963}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.022410588338971138}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.02229095809161663}\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0223\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.022150477394461632}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.02201470360159874}\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0220\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.021877555176615715}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.02175358682870865}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.021608348935842514}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.021485883742570877}\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0215\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.021359601989388466}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.021242642775177956}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.021235866472125053}\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0212\n",
      " For Epoch Number 7 the model has loss[mean_absolute_error] of 0.021235866472125053\n",
      "43/43 [==============================] - 1s 22ms/step - loss: 0.0212 - val_loss: 0.0022 - lr: 7.4082e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0006703201215714216.\n",
      "Epoch 8/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.016204889863729477}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0162\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.01589866355061531}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.01596038043498993}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.01570071466267109}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.015537818893790245}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0155\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.015457156114280224}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.015281634405255318}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.01524829026311636}\n",
      " 8/43 [====>.........................] - ETA: 0s - loss: 0.0152\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.015141114592552185}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.015019232407212257}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.014926876872777939}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.014763073064386845}\n",
      "12/43 [=======>......................] - ETA: 0s - loss: 0.0148\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.01460251770913601}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.014499773271381855}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.014393937774002552}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.014282695949077606}\n",
      "16/43 [==========>...................] - ETA: 0s - loss: 0.0143\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.01418235432356596}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.014067964628338814}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.013938646763563156}\n",
      "19/43 [============>.................] - ETA: 0s - loss: 0.0139\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.013820339925587177}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.013682444579899311}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0135867390781641}\n",
      "22/43 [==============>...............] - ETA: 0s - loss: 0.0136\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.013475535437464714}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.013372492045164108}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.013249889947474003}\n",
      "25/43 [================>.............] - ETA: 0s - loss: 0.0132\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.01316539105027914}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.013058213517069817}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.012939438223838806}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.012834635563194752}\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 0.0128\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.01273749303072691}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.01263844221830368}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.012535875663161278}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.012429274618625641}\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0124\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.012330622412264347}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.012231376953423023}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.012122421525418758}\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0121\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.012005523778498173}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.01190313883125782}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.011805955320596695}\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0118\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.01170424371957779}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.011606797575950623}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.01150432601571083}\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0115\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.011499880813062191}\n",
      "\n",
      " For Epoch Number 8 the model has loss[mean_absolute_error] of 0.011499880813062191\n",
      "43/43 [==============================] - 1s 22ms/step - loss: 0.0115 - val_loss: 0.0021 - lr: 6.7032e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x135baeed850>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_snn.fit(dep_train, indep_train, validation_data=(dep_val,indep_val),batch_size = 1000, epochs = 8, callbacks=[schedular_callback,LossCallback(), csv_callback,tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 1s 4ms/step - loss: 0.0022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.002162031829357147"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_snn.evaluate(dep_test, indep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 2s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0103789 ],\n",
       "       [0.01052278],\n",
       "       [0.01038009],\n",
       "       ...,\n",
       "       [0.01052535],\n",
       "       [0.01054573],\n",
       "       [0.01016152]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_snn.predict(dep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17416), started 5 days, 1:03:26 ago. (Use '!kill 17416' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9362ecab5426915c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9362ecab5426915c\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  # model = tf.keras.Sequential()\n",
    "  # model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "  # hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
    "  hp_layer_1 = hp.Int('layer_1', min_value=1, max_value=1000, step=100)\n",
    "  # hp_layer_2 = hp.Int('layer_2', min_value=1, max_value=1000, step=100)\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "#   model.add(tf.keras.layers.Dense(units=hp_layer_1, activation=hp_activation))\n",
    "#   model.add(tf.keras.layers.Dense(units=hp_layer_2, activation=hp_activation))\n",
    "#   model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "  model= SobolevNetwork(input_dim=16,num_hidden=hp_layer_1)\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='mean_absolute_error')\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project dir\\x\\oracle.json\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     \n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='dir',\n",
    "                     project_name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "901               |?                 |layer_1\n",
      "0.01              |?                 |learning_rate\n",
      "2                 |?                 |tuner/epochs\n",
      "0                 |?                 |tuner/initial_epoch\n",
      "2                 |?                 |tuner/bracket\n",
      "0                 |?                 |tuner/round\n",
      "\n",
      "Epoch 1/2\n",
      "1051/1051 [==============================] - 90s 83ms/step - loss: 1.5470 - val_loss: 0.0431\n",
      "Epoch 2/2\n",
      "1051/1051 [==============================] - 93s 89ms/step - loss: 0.0095 - val_loss: 0.0036\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\bro\\Analysis\\mSANN\\cd\\test\\final.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(dep_train, indep_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[es_callback])\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:203\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m         tuner_utils\u001b[39m.\u001b[39mvalidate_trial_results(\n\u001b[0;32m    197\u001b[0m             results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective, \u001b[39m\"\u001b[39m\u001b[39mTuner.run_trial()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         ),\n\u001b[0;32m    199\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mupdate_trial(\n\u001b[0;32m    200\u001b[0m             trial\u001b[39m.\u001b[39mtrial_id,\n\u001b[0;32m    201\u001b[0m             \u001b[39m# Convert to dictionary before calling `update_trial()`\u001b[39;00m\n\u001b[0;32m    202\u001b[0m             \u001b[39m# to pass it from gRPC.\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m             tuner_utils\u001b[39m.\u001b[39;49mconvert_to_metrics_dict(\n\u001b[0;32m    204\u001b[0m                 results,\n\u001b[0;32m    205\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moracle\u001b[39m.\u001b[39;49mobjective,\n\u001b[0;32m    206\u001b[0m             ),\n\u001b[0;32m    207\u001b[0m             step\u001b[39m=\u001b[39mtuner_utils\u001b[39m.\u001b[39mget_best_step(results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective),\n\u001b[0;32m    208\u001b[0m         )\n\u001b[0;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner_utils.py:266\u001b[0m, in \u001b[0;36mconvert_to_metrics_dict\u001b[1;34m(results, objective)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39m# List of multiple exectuion results to be averaged.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m# Check this case first to deal each case individually to check for errors.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m average_metrics_dicts(\n\u001b[1;32m--> 266\u001b[0m         [convert_to_metrics_dict(elem, objective) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m results]\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    269\u001b[0m \u001b[39m# Single value.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results, (\u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m, np\u001b[39m.\u001b[39mfloating)):\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner_utils.py:266\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39m# List of multiple exectuion results to be averaged.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m# Check this case first to deal each case individually to check for errors.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m average_metrics_dicts(\n\u001b[1;32m--> 266\u001b[0m         [convert_to_metrics_dict(elem, objective) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m results]\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    269\u001b[0m \u001b[39m# Single value.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results, (\u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m, np\u001b[39m.\u001b[39mfloating)):\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner_utils.py:279\u001b[0m, in \u001b[0;36mconvert_to_metrics_dict\u001b[1;34m(results, objective)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39m# A History.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results, keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mHistory):\n\u001b[1;32m--> 279\u001b[0m     best_value, _ \u001b[39m=\u001b[39m _get_best_value_and_best_epoch_from_history(\n\u001b[0;32m    280\u001b[0m         results, objective\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m best_value\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner_utils.py:250\u001b[0m, in \u001b[0;36m_get_best_value_and_best_epoch_from_history\u001b[1;34m(history, objective)\u001b[0m\n\u001b[0;32m    248\u001b[0m best_epoch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    249\u001b[0m \u001b[39mfor\u001b[39;00m epoch, metrics \u001b[39min\u001b[39;00m epoch_metrics\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 250\u001b[0m     objective_value \u001b[39m=\u001b[39m objective\u001b[39m.\u001b[39;49mget_value(metrics)\n\u001b[0;32m    251\u001b[0m     \u001b[39m# Support multi-objective.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m objective\u001b[39m.\u001b[39mname \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m metrics:\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\objective.py:55\u001b[0m, in \u001b[0;36mObjective.get_value\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_value\u001b[39m(\u001b[39mself\u001b[39m, logs):\n\u001b[0;32m     45\u001b[0m     \u001b[39m\"\"\"Get the objective value from the metrics logs.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39m        The objective value.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m logs[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mse'"
     ]
    }
   ],
   "source": [
    "tuner.search(dep_train, indep_train, epochs=50, validation_split=0.2, callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57026/57026 [==============================] - 0s 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((404, 13), (102, 13), (404,), (102,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "(X_train_reg, Y_train_reg), (X_test_reg, Y_test_reg) = datasets.boston_housing.load_data()\n",
    "\n",
    "X_train_reg.shape, X_test_reg.shape, Y_train_reg.shape, Y_test_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "def build_model(hyperparams):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(X_train_reg.shape[1],)))\n",
    "    model.add(layers.Dense(units=hyperparams.Int(\"units_l1\", 16, 50, step=16),\n",
    "                           use_bias=hyperparams.Boolean(\"bias_l1\"),\n",
    "                           activation=hyperparams.Choice(\"act_l1\", [\"relu\", \"tanh\"])\n",
    "                          ))\n",
    "    model.add(layers.Dense(units=hyperparams.Int(\"units_l2\", 16, 50, step=16),\n",
    "                           use_bias=hyperparams.Boolean(\"bias_l2\"),\n",
    "                           activation=hyperparams.Choice(\"act_l2\", [\"relu\", \"tanh\"])\n",
    "                          ))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    optim=hyperparams.Choice(\"optimizer\",[\"sgd\",\"rmsprop\",\"adam\"])\n",
    "    model.compile(optim, loss=\"mean_squared_error\", metrics=[\"mean_squared_error\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 07s]\n",
      "val_mean_squared_error: 310.8663635253906\n",
      "\n",
      "Best val_mean_squared_error So Far: 70.79254913330078\n",
      "Total elapsed time: 00h 00m 36s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "from keras_tuner import RandomSearch\n",
    "from keras_tuner import Objective\n",
    "\n",
    "tuner1 =  RandomSearch(hypermodel=build_model,\n",
    "                      objective=\"val_mean_squared_error\",\n",
    "                      #objective=Objective(name=\"val_mean_squared_error\",direction=\"min\"),\n",
    "                      max_trials=5,\n",
    "                      #seed=123,\n",
    "                      project_name=\"Regression\",\n",
    "                      overwrite=True\n",
    "                    )\n",
    "\n",
    "tuner1.search(X_train_reg, Y_train_reg, batch_size=32, epochs=10, validation_data=(X_test_reg, Y_test_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'units_l1': 32,\n",
       " 'bias_l1': True,\n",
       " 'act_l1': 'tanh',\n",
       " 'units_l2': 32,\n",
       " 'bias_l2': True,\n",
       " 'act_l2': 'tanh',\n",
       " 'optimizer': 'sgd'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = tuner1.get_best_hyperparameters()\n",
    "\n",
    "best_params[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                448       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,537\n",
      "Trainable params: 1,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner1.get_best_models()[0]\n",
    "\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERBAND_MAX_EPOCHS = 150\n",
    "EXECUTION_PER_TRIAL = 2\n",
    "from keras_tuner.tuners import RandomSearch, Hyperband\n",
    "tuner= Hyperband(build_model,\n",
    "                   objective= 'val_mean_squared_error',\n",
    "                   max_epochs=HYPERBAND_MAX_EPOCHS, #Set 100+ for good results\n",
    "                   executions_per_trial=EXECUTION_PER_TRIAL,\n",
    "                   directory= 'hyperband',\n",
    "                   project_name='houseprices',\n",
    "                   overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\metrics_tracking.py:105: RuntimeWarning: All-NaN axis encountered\n",
      "  return np.nanmin(values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0008s vs `on_train_batch_begin` time: 0.0015s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0014s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0013s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0013s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0005s vs `on_train_batch_end` time: 0.0015s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0013s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0012s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0004s). Check your callbacks.\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tuner.search(x= X_train_reg,\n",
    "             y= Y_train_reg,\n",
    "             epochs=100,\n",
    "             batch_size= 64,\n",
    "             validation_data= (X_test_reg, Y_test_reg),\n",
    "             verbose=0,\n",
    "             \n",
    "            )\n",
    "# tuner1.search(X_train_reg, Y_train_reg, batch_size=32, epochs=10, validation_data=(X_test_reg, Y_test_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras_tuner.engine.hyperparameters.HyperParameters at 0x135bb288820>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.momentum\n"
     ]
    }
   ],
   "source": [
    "# Retreive the optimal hyperparameters\n",
    "best_hps= tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the \n",
      "first densely-connected layer is 32,\n",
      "second layer is 32 \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Creating variables on a non-first call to a function decorated with tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\bro\\Analysis\\mSANN\\cd\\test\\final.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Evaluate the best model.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(best_model\u001b[39m.\u001b[39mmetrics_names)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss, mae, mse \u001b[39m=\u001b[39m best_model\u001b[39m.\u001b[39;49mevaluate(X_test_reg, Y_test_reg)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X42sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss:\u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m mae: \u001b[39m\u001b[39m{\u001b[39;00mmae\u001b[39m}\u001b[39;00m\u001b[39m mse: \u001b[39m\u001b[39m{\u001b[39;00mmse\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:956\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    954\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m--> 956\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    958\u001b[0m   \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m    960\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m   \u001b[39m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Creating variables on a non-first call to a function decorated with tf.function."
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the \n",
    "first densely-connected layer is {best_hps.get('units_l1')},\n",
    "second layer is {best_hps.get('units_l2')} \n",
    "  \n",
    "\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Evaluate the best model.\n",
    "print(best_model.metrics_names)\n",
    "loss, mae, mse = best_model.evaluate(X_test_reg, Y_test_reg)\n",
    "print(f'loss:{loss} mae: {mae} mse: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13bec6b3e23dd0921d86a1ec88a5a342f8b423eb92f661fc2c47e8b6a92d2fe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
