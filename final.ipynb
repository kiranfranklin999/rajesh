{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import random,os,io\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint,CSVLogger\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42039, 16), (42039, 1), (10935, 16), (10935, 1), (10934, 16), (10934, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dep_train=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XTr.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_train=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yTr.dat\",sep='\\s+',names=['target'])\n",
    "dep_val=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XV.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_val=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yV.dat\",sep='\\s+',names=[\"target\"])\n",
    "dep_test=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XT.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_test=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yT.dat\",sep='\\s+',names=[\"target\"])\n",
    "\n",
    "dep_train.shape,indep_train.shape,dep_val.shape,indep_val.shape,dep_test.shape,indep_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SobolevNetwork(Model):\n",
    "    def __init__(self, input_dim, num_hidden,init = None):\n",
    "        super(SobolevNetwork, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.num_hidden = num_hidden\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.input_dim, self.num_hidden],stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.W4 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b4 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.W5 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b5 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.W6 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b6 = tf.Variable(tf.ones([self.num_hidden]))        \n",
    "        self.W7 = tf.Variable(tf.random.normal([self.num_hidden, 1],stddev=0.1))\n",
    "        self.b7 = tf.Variable(tf.ones([1]))\n",
    "        self.w = [(self.W1, self.b1), (self.W2, self.b2), (self.W3, self.b3),(self.W4, self.b4), (self.W5, self.b5), (self.W6, self.b6),(self.W7, self.b7)]\n",
    "        \n",
    "    def call(self, X):\n",
    "        #Input layer\n",
    "        out = X\n",
    "        #Hidden layers\n",
    "        W,b = self.w[0]\n",
    "        out = tf.nn.tanh(tf.matmul(out, W) + b)\n",
    "        W,b = self.w[1]\n",
    "        out = tf.nn.tanh(tf.matmul(out, W) + b)\n",
    "        W,b = self.w[2]\n",
    "        out = tf.nn.sigmoid(tf.matmul(out, W) + b)\n",
    "        W,b = self.w[3]\n",
    "        out = tf.nn.sigmoid(tf.matmul(out, W) + b)\n",
    "        W,b = self.w[4]\n",
    "        out = tf.nn.leaky_relu(tf.matmul(out, W) + b)\n",
    "        W,b = self.w[5]\n",
    "        out = tf.nn.relu(tf.matmul(out, W) + b)\n",
    "        #Output layer\n",
    "        W,b = self.w[-1]\n",
    "        out = tf.matmul(out, W) + b\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\n For Epoch Number {} the model has loss[mean_absolute_error] of {}\".format(epoch+1, logs[\"loss\"]))\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        print(\"\\n For Batch Number {} the model has loss[mean_absolute_error] of {}\".format(batch+1, logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_callback = CSVLogger( 'logs.csv',separator=',',append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping( monitor='val_loss',min_delta=0,patience=2, verbose=1, mode=\"auto\", baseline=None, restore_best_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedular(epoch, lr):\n",
    "    if epoch <= 3:\n",
    "        learning_rate = lr\n",
    "    else:\n",
    "        learning_rate = lr * tf.math.exp(-0.1)\n",
    "        learning_rate = learning_rate.numpy()\n",
    "    \n",
    "\n",
    "    # with train_writer.as_default():\n",
    "    #     tf.summary.scalar('LearningRateScheduler', data= learning_rate, step= epoch)\n",
    "    \n",
    "    return learning_rate\n",
    "\n",
    "schedular_callback = LearningRateScheduler(schedular, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_1 = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_snn= SobolevNetwork(input_dim=16,num_hidden=num_hidden_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_snn.compile(optimizer=tf.keras.optimizers.Adam(),  \n",
    "            loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0006703201215714216.\n",
      "Epoch 1/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0021293265745043755}\n",
      " 1/43 [..............................] - ETA: 16s - loss: 0.0021\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0020510097965598106}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.0020535450894385576}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.002038370817899704}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0020624068565666676}\n",
      "\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.002063472056761384}\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_train_batch_end` time: 0.0660s). Check your callbacks.\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0020665598567575216}\n",
      " 7/43 [===>..........................] - ETA: 0s - loss: 0.0021 \n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.002076128264889121}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0020574242807924747}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.00208544940687716}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0020878203213214874}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.002089183311909437}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.0020797501783818007}\n",
      "13/43 [========>.....................] - ETA: 0s - loss: 0.0021\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.002092601265758276}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.0020943772979080677}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0020905009005218744}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.00209577614441514}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0020859302021563053}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.002079376485198736}\n",
      "19/43 [============>.................] - ETA: 0s - loss: 0.0021\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.0020835225004702806}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.002084096660837531}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0020855474285781384}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.002096037147566676}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0020909106824547052}\n",
      "24/43 [===============>..............] - ETA: 0s - loss: 0.0021\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0020914424676448107}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0020963617134839296}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.0020958464592695236}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.00209436216391623}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.002088188426569104}\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 0.0021\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.002084260107949376}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.00208421447314322}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.0020811562426388264}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0020828023552894592}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0020839397329837084}\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0021\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.0020772453863173723}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.0020760984625667334}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0020728246308863163}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.002071423688903451}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.002071891911327839}\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0021\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.002073554787784815}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.002069438574835658}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0020669230725616217}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.002067004796117544}\n",
      "\n",
      " For Epoch Number 1 the model has loss[mean_absolute_error] of 0.002067004796117544\n",
      "43/43 [==============================] - 1s 20ms/step - loss: 0.0021 - val_loss: 0.0024 - lr: 6.7032e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0006703201215714216.\n",
      "Epoch 2/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0022886074148118496}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0023\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0022755316458642483}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.002227021614089608}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.002220111433416605}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.002224247669801116}\n",
      "\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.00223812204785645}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0021932919044047594}\n",
      " 7/43 [===>..........................] - ETA: 0s - loss: 0.0022\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.002195006003603339}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0021740433294326067}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.0021705548278987408}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.002162075834348798}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0021485835313796997}\n",
      "12/43 [=======>......................] - ETA: 0s - loss: 0.0021\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.002131343586370349}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.0021195239387452602}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.002118693431839347}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0021115548443049192}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.002105507766827941}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.002107654232531786}\n",
      "18/43 [===========>..................] - ETA: 0s - loss: 0.0021\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0021078940480947495}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.0021032115910202265}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0020975880324840546}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0020863967947661877}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.002081839134916663}\n",
      "23/43 [===============>..............] - ETA: 0s - loss: 0.0021\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.00207980046980083}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0020725408103317022}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.002064724452793598}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.0020631272345781326}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0020621109288185835}\n",
      "28/43 [==================>...........] - ETA: 0s - loss: 0.0021\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.002060905797407031}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0020590804051607847}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.0020565956365317106}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.0020534370560199022}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0020477320067584515}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.002044804859906435}\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0020\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.002046392299234867}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.002043061889708042}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0020380697678774595}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.002035469049587846}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.002037924248725176}\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0020\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0020355896558612585}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0020347139798104763}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0020354408770799637}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.002035552402958274}\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0020\n",
      " For Epoch Number 2 the model has loss[mean_absolute_error] of 0.002035552402958274\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0020 - val_loss: 0.0022 - lr: 6.7032e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0006703201215714216.\n",
      "Epoch 3/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.002497961977496743}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.002204696647822857}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.002191872103139758}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.002150558866560459}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0020998690742999315}\n",
      "\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0021076700650155544}\n",
      " 6/43 [===>..........................] - ETA: 0s - loss: 0.0021\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0021035473328083754}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0020795816089957952}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.002069798531010747}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.002063323510810733}\n",
      "10/43 [=====>........................] - ETA: 0s - loss: 0.0021\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.002052915748208761}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0020442684181034565}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.002031841780990362}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.0020325835794210434}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.0020271565299481153}\n",
      "15/43 [=========>....................] - ETA: 0s - loss: 0.0020\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.002021134365350008}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.0020041035022586584}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0019882291089743376}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0019889697432518005}\n",
      "19/43 [============>.................] - ETA: 0s - loss: 0.0020\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.001977195730432868}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0019814050756394863}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.001971740974113345}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0019691488705575466}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0019714003428816795}\n",
      "24/43 [===============>..............] - ETA: 0s - loss: 0.0020\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.001966066425666213}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.001969022210687399}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.0019639397505670786}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.001964194467291236}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.0019603725522756577}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0019596731290221214}\n",
      "30/43 [===================>..........] - ETA: 0s - loss: 0.0020\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.001959925051778555}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.0019598130602389574}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0019531836733222008}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0019482903880998492}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.0019449666142463684}\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0019\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.001939179259352386}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0019361727172508836}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.001935278414748609}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.0019311925861984491}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0019311184296384454}\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0019\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0019339793361723423}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0019297392573207617}\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0019\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.001929599791765213}\n",
      "\n",
      " For Epoch Number 3 the model has loss[mean_absolute_error] of 0.001929599791765213\n",
      "43/43 [==============================] - 1s 22ms/step - loss: 0.0019 - val_loss: 0.0019 - lr: 6.7032e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0006703201215714216.\n",
      "Epoch 4/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0018937927670776844}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0019\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0018656653119251132}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.0018989653326570988}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.0018791561014950275}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0018722070381045341}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0019\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0018568799132481217}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0018548194784671068}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0018641235074028373}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0018652596045285463}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.0018608103273436427}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0018514801049605012}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0018665660172700882}\n",
      "12/43 [=======>......................] - ETA: 0s - loss: 0.0019\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.0018613035790622234}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.0018521937308833003}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.001849775668233633}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0018466915935277939}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.0018416529055684805}\n",
      "17/43 [==========>...................] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0018453269731253386}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.001845170627348125}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.0018459755228832364}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0018496280536055565}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0018469695933163166}\n",
      "22/43 [==============>...............] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0018489154754206538}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0018487338675186038}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0018424976151436567}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0018414059886708856}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.0018369712634012103}\n",
      "27/43 [=================>............] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.001836355309933424}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.0018289966974407434}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0018279419746249914}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.0018251839792355895}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.0018269983120262623}\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0018258680356666446}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0018265695543959737}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.0018276831833645701}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.001826970255933702}\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0018288380233570933}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.0018299336079508066}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.001833108952268958}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0018299889052286744}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.001831648638471961}\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0018301389645785093}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.001829976332373917}\n",
      "\n",
      " For Epoch Number 4 the model has loss[mean_absolute_error] of 0.001829976332373917\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0018 - val_loss: 0.0018 - lr: 6.7032e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0006065307534299791.\n",
      "Epoch 5/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0018179583130404353}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.001799871795810759}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.0018510586814954877}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.001835240749642253}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0018122869078069925}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0017868299037218094}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0017928616143763065}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0017702049808576703}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0017699080053716898}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.0017748050158843398}\n",
      "10/43 [=====>........................] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0017787128454074264}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0017788992263376713}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.0017837447812780738}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.0017775553278625011}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.0017930917674675584}\n",
      "15/43 [=========>....................] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0017950487090274692}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.0018009620252996683}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0017916200449690223}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0017956593073904514}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.001794935204088688}\n",
      "20/43 [============>.................] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.001790597103536129}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.001799231395125389}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0017947994638234377}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0017973268404603004}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0018003437435254455}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0018045952310785651}\n",
      "26/43 [=================>............] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.0018074200488626957}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0018056232947856188}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.0018002238357439637}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0017978925025090575}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.0017951795598492026}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.00179606257006526}\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0017929301830008626}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0017915073549374938}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.0017928957240656018}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.0017873672768473625}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0017897062934935093}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.0017834851751103997}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.0017821978544816375}\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0017833682941272855}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0017816325416788459}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0017779688350856304}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.0017779750050976872}\n",
      "\n",
      " For Epoch Number 5 the model has loss[mean_absolute_error] of 0.0017779750050976872\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.0018 - val_loss: 0.0020 - lr: 6.0653e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0005488117458298802.\n",
      "Epoch 6/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.001866689184680581}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0019\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.001811306457966566}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.0018024262972176075}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.0017758297035470605}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0017621071310713887}\n",
      "\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0017420134972780943}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.001736550242640078}\n",
      " 7/43 [===>..........................] - ETA: 0s - loss: 0.0017\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0017467814031988382}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0017319433391094208}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.001725922804325819}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0017212439561262727}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0017183523159474134}\n",
      "12/43 [=======>......................] - ETA: 0s - loss: 0.0017\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.0017183915479108691}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.0017245396738871932}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.001724936068058014}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0017235885607078671}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.001728755421936512}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0017310866387560964}\n",
      "18/43 [===========>..................] - ETA: 0s - loss: 0.0017\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0017248444491997361}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.0017162504373118281}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0017096295487135649}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0017054928466677666}\n",
      "22/43 [==============>...............] - ETA: 0s - loss: 0.0017\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0017007555579766631}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0016951747238636017}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0016963608795776963}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0016934358282014728}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.0016921107890084386}\n",
      "27/43 [=================>............] - ETA: 0s - loss: 0.0017\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0016926516545936465}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.001695717335678637}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0016935286112129688}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.001697723288089037}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.0016991306329146028}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0016983880195766687}\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0017\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.001695839804597199}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.0016905064694583416}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.001685479306615889}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0016798361903056502}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.0016803968464955688}\n",
      "38/43 [=========================>....] - ETA: 0s - loss: 0.0017\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.0016803594771772623}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0016781575977802277}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0016747332410886884}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0016701819840818644}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.0016705516027286649}\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0017\n",
      " For Epoch Number 6 the model has loss[mean_absolute_error] of 0.0016705516027286649\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.0017 - val_loss: 0.0018 - lr: 5.4881e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0004965853877365589.\n",
      "Epoch 7/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0018406843300908804}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0018\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.001700756256468594}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.0017012165626510978}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.0016824750928208232}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.001676417188718915}\n",
      "\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0016824549529701471}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0016704562585800886}\n",
      " 7/43 [===>..........................] - ETA: 0s - loss: 0.0017\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0016741560539230704}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0016711550997570157}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.0016691688215360045}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.001675364444963634}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0016627010190859437}\n",
      "12/43 [=======>......................] - ETA: 0s - loss: 0.0017\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.0016502932412549853}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.0016489721601828933}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.001646114862523973}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0016412932891398668}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.0016424049390479922}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0016322577139362693}\n",
      "18/43 [===========>..................] - ETA: 0s - loss: 0.0016\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0016335841501131654}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.0016304260352626443}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0016290027415379882}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0016180286183953285}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.001614605775102973}\n",
      "23/43 [===============>..............] - ETA: 0s - loss: 0.0016\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0016089386772364378}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0016018148744478822}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.001602271688170731}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.0016054834704846144}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.001603110576979816}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.0015966257778927684}\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 0.0016\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0015969773521646857}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.0015994410496205091}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.001593979774042964}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0015986980870366096}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.001599430455826223}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.001600656658411026}\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0016\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.001594981295056641}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0015943138860166073}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.0015947914216667414}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.0015847345348447561}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0015859358245506883}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0015837551327422261}\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0016\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0015850510681048036}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.001584729878231883}\n",
      "\n",
      " For Epoch Number 7 the model has loss[mean_absolute_error] of 0.001584729878231883\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.0016 - val_loss: 0.0014 - lr: 4.9659e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0004493290325626731.\n",
      "Epoch 8/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.001652503153309226}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0017\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0016077797627076507}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.0015653470763936639}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.0015193633735179901}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0015147947706282139}\n",
      "\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0015236175386235118}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.001513787778094411}\n",
      " 7/43 [===>..........................] - ETA: 0s - loss: 0.0015\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0015114996349439025}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0014916039071977139}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.0014672601828351617}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0014738753670826554}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0014671975513920188}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.0014659750740975142}\n",
      "13/43 [========>.....................] - ETA: 0s - loss: 0.0015\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.0014565425226464868}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.0014544924488291144}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0014512763591483235}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.0014435582561418414}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0014432074967771769}\n",
      "18/43 [===========>..................] - ETA: 0s - loss: 0.0014\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.00145023874938488}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.0014425007393583655}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0014391072327271104}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0014303686330094934}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0014205435290932655}\n",
      "23/43 [===============>..............] - ETA: 0s - loss: 0.0014\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0014167454792186618}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0014081395929679275}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0014034028863534331}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.0014076981460675597}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0014074641512706876}\n",
      "28/43 [==================>...........] - ETA: 0s - loss: 0.0014\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.0014065399300307035}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0014005966950207949}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.0013966062106192112}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.0013962701195850968}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0013922764919698238}\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0014\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0013939037453383207}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.0013932852307334542}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.001392156700603664}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0013877998571842909}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.001381186069920659}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.0013762047747150064}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0013718042755499482}\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0014\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0013671949272975326}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0013616278301924467}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.001361922244541347}\n",
      "\n",
      " For Epoch Number 8 the model has loss[mean_absolute_error] of 0.001361922244541347\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.0014 - val_loss: 0.0012 - lr: 4.4933e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bb97fd3760>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_snn.fit(dep_train, indep_train, validation_data=(dep_val,indep_val),batch_size = 1000, epochs = 8, callbacks=[schedular_callback,LossCallback(), csv_callback,tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 2s 4ms/step - loss: 0.0022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.002203913638368249"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_snn.evaluate(dep_test, indep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 2s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0103789 ],\n",
       "       [0.01052278],\n",
       "       [0.01038009],\n",
       "       ...,\n",
       "       [0.01052535],\n",
       "       [0.01054573],\n",
       "       [0.01016152]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_snn.predict(dep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17416), started 0:06:31 ago. (Use '!kill 17416' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-459289e11feaf1af\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-459289e11feaf1af\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter-tensorboard\n",
      "  Downloading jupyter_tensorboard-0.2.0.tar.gz (15 kB)\n",
      "Requirement already satisfied: notebook>=5.0 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from jupyter-tensorboard) (6.4.8)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (6.1.12)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (0.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (2.11.3)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (4.9.2)\n",
      "Requirement already satisfied: nbformat in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (5.3.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (21.3.0)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (22.3.0)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (5.1.1)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (6.9.1)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (6.4.4)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (0.13.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (1.5.5)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (1.8.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (6.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from notebook>=5.0->jupyter-tensorboard) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from jupyter-client>=5.3.4->notebook>=5.0->jupyter-tensorboard) (2.8.2)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from jupyter-core>=4.6.1->notebook>=5.0->jupyter-tensorboard) (302)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->jupyter-client>=5.3.4->notebook>=5.0->jupyter-tensorboard) (1.16.0)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=5.0->jupyter-tensorboard) (2.0.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook>=5.0->jupyter-tensorboard) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=5.0->jupyter-tensorboard) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=5.0->jupyter-tensorboard) (2.21)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from ipykernel->notebook>=5.0->jupyter-tensorboard) (0.1.2)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from ipykernel->notebook>=5.0->jupyter-tensorboard) (8.2.0)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from ipykernel->notebook>=5.0->jupyter-tensorboard) (1.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.4.4)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->notebook>=5.0->jupyter-tensorboard) (2.11.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->notebook>=5.0->jupyter-tensorboard) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->notebook>=5.0->jupyter-tensorboard) (3.0.20)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->notebook>=5.0->jupyter-tensorboard) (61.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.18.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.2.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.7.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from jinja2->notebook>=5.0->jupyter-tensorboard) (2.0.1)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (0.7.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (0.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (0.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (1.5.0)\n",
      "Requirement already satisfied: bleach in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (4.1.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (0.5.13)\n",
      "Requirement already satisfied: testpath in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (0.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (0.8.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.0->jupyter-tensorboard) (4.11.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from nbformat->notebook>=5.0->jupyter-tensorboard) (2.15.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from nbformat->notebook>=5.0->jupyter-tensorboard) (4.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat->notebook>=5.0->jupyter-tensorboard) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat->notebook>=5.0->jupyter-tensorboard) (21.4.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert->notebook>=5.0->jupyter-tensorboard) (2.3.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=5.0->jupyter-tensorboard) (21.3)\n",
      "Requirement already satisfied: webencodings in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=5.0->jupyter-tensorboard) (0.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from packaging->bleach->nbconvert->notebook>=5.0->jupyter-tensorboard) (3.0.4)\n",
      "Requirement already satisfied: asttokens in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->notebook>=5.0->jupyter-tensorboard) (2.0.5)\n",
      "Requirement already satisfied: executing in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\kiran.franklin\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->notebook>=5.0->jupyter-tensorboard) (0.2.2)\n",
      "Building wheels for collected packages: jupyter-tensorboard\n",
      "  Building wheel for jupyter-tensorboard (setup.py): started\n",
      "  Building wheel for jupyter-tensorboard (setup.py): finished with status 'done'\n",
      "  Created wheel for jupyter-tensorboard: filename=jupyter_tensorboard-0.2.0-py2.py3-none-any.whl size=15250 sha256=8491c90c641169a2c30aa22d51d76772dd27332499bda808bd26bf1f646a783f\n",
      "  Stored in directory: c:\\users\\kiran.franklin\\appdata\\local\\pip\\cache\\wheels\\98\\03\\14\\670a2eefd86be0e21a529c469cadc2148cbeb00d82937e5aa2\n",
      "Successfully built jupyter-tensorboard\n",
      "Installing collected packages: jupyter-tensorboard\n",
      "Successfully installed jupyter-tensorboard-0.2.0\n"
     ]
    }
   ],
   "source": [
    "pip install jupyter-tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13bec6b3e23dd0921d86a1ec88a5a342f8b423eb92f661fc2c47e8b6a92d2fe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
