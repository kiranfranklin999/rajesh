{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import random,os,io\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint,CSVLogger\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42039, 16), (42039, 1), (10935, 16), (10935, 1), (10934, 16), (10934, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dep_train=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XTr.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_train=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yTr.dat\",sep='\\s+',names=['target'])\n",
    "dep_val=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XV.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_val=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yV.dat\",sep='\\s+',names=[\"target\"])\n",
    "dep_test=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XT.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_test=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yT.dat\",sep='\\s+',names=[\"target\"])\n",
    "\n",
    "dep_train.shape,indep_train.shape,dep_val.shape,indep_val.shape,dep_test.shape,indep_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "      <td>42039.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.248356</td>\n",
       "      <td>-0.004138</td>\n",
       "      <td>-0.002063</td>\n",
       "      <td>-0.000411</td>\n",
       "      <td>-0.000244</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.955813</td>\n",
       "      <td>0.001766</td>\n",
       "      <td>-0.002187</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>0.451158</td>\n",
       "      <td>2.030716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.143186</td>\n",
       "      <td>0.017691</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.314483</td>\n",
       "      <td>0.040846</td>\n",
       "      <td>0.019695</td>\n",
       "      <td>0.008817</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.086088</td>\n",
       "      <td>1.827649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.497415</td>\n",
       "      <td>-0.136435</td>\n",
       "      <td>-0.038454</td>\n",
       "      <td>-0.014248</td>\n",
       "      <td>-0.008885</td>\n",
       "      <td>-0.006627</td>\n",
       "      <td>-0.003817</td>\n",
       "      <td>-1.499592</td>\n",
       "      <td>-0.227272</td>\n",
       "      <td>-0.101638</td>\n",
       "      <td>-0.059503</td>\n",
       "      <td>-0.036878</td>\n",
       "      <td>-0.014407</td>\n",
       "      <td>-0.009225</td>\n",
       "      <td>0.300213</td>\n",
       "      <td>-1.198834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.371456</td>\n",
       "      <td>-0.015947</td>\n",
       "      <td>-0.006744</td>\n",
       "      <td>-0.002432</td>\n",
       "      <td>-0.001656</td>\n",
       "      <td>-0.000697</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>-1.227219</td>\n",
       "      <td>-0.023441</td>\n",
       "      <td>-0.016485</td>\n",
       "      <td>-0.007183</td>\n",
       "      <td>-0.003799</td>\n",
       "      <td>-0.001689</td>\n",
       "      <td>-0.001545</td>\n",
       "      <td>0.377545</td>\n",
       "      <td>0.466435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.247950</td>\n",
       "      <td>-0.003717</td>\n",
       "      <td>-0.002128</td>\n",
       "      <td>-0.000507</td>\n",
       "      <td>-0.000342</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.956767</td>\n",
       "      <td>0.004604</td>\n",
       "      <td>-0.004193</td>\n",
       "      <td>-0.001187</td>\n",
       "      <td>-0.000539</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>-0.000227</td>\n",
       "      <td>0.452744</td>\n",
       "      <td>2.031259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.125162</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>-0.686958</td>\n",
       "      <td>0.030148</td>\n",
       "      <td>0.010435</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.524987</td>\n",
       "      <td>3.600611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.065562</td>\n",
       "      <td>0.038597</td>\n",
       "      <td>0.019244</td>\n",
       "      <td>0.012764</td>\n",
       "      <td>0.005687</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>-0.403419</td>\n",
       "      <td>0.300085</td>\n",
       "      <td>0.097836</td>\n",
       "      <td>0.037676</td>\n",
       "      <td>0.017639</td>\n",
       "      <td>0.022246</td>\n",
       "      <td>0.008829</td>\n",
       "      <td>0.599638</td>\n",
       "      <td>5.194131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4  \\\n",
       "count  42039.000000  42039.000000  42039.000000  42039.000000  42039.000000   \n",
       "mean      -0.248356     -0.004138     -0.002063     -0.000411     -0.000244   \n",
       "std        0.143186      0.017691      0.006818      0.002907      0.002112   \n",
       "min       -0.497415     -0.136435     -0.038454     -0.014248     -0.008885   \n",
       "25%       -0.371456     -0.015947     -0.006744     -0.002432     -0.001656   \n",
       "50%       -0.247950     -0.003717     -0.002128     -0.000507     -0.000342   \n",
       "75%       -0.125162      0.007631      0.002595      0.001448      0.001014   \n",
       "max        0.000064      0.065562      0.038597      0.019244      0.012764   \n",
       "\n",
       "                  5             6             7             8             9  \\\n",
       "count  42039.000000  42039.000000  42039.000000  42039.000000  42039.000000   \n",
       "mean       0.000038      0.000032     -0.955813      0.001766     -0.002187   \n",
       "std        0.001049      0.000804      0.314483      0.040846      0.019695   \n",
       "min       -0.006627     -0.003817     -1.499592     -0.227272     -0.101638   \n",
       "25%       -0.000697     -0.000500     -1.227219     -0.023441     -0.016485   \n",
       "50%       -0.000014      0.000057     -0.956767      0.004604     -0.004193   \n",
       "75%        0.000713      0.000558     -0.686958      0.030148      0.010435   \n",
       "max        0.005687      0.007473     -0.403419      0.300085      0.097836   \n",
       "\n",
       "                 10            11            12            13            14  \\\n",
       "count  42039.000000  42039.000000  42039.000000  42039.000000  42039.000000   \n",
       "mean      -0.001022     -0.000561      0.000159     -0.000144      0.451158   \n",
       "std        0.008817      0.004493      0.002566      0.001893      0.086088   \n",
       "min       -0.059503     -0.036878     -0.014407     -0.009225      0.300213   \n",
       "25%       -0.007183     -0.003799     -0.001689     -0.001545      0.377545   \n",
       "50%       -0.001187     -0.000539      0.000047     -0.000227      0.452744   \n",
       "75%        0.004850      0.002828      0.001818      0.001189      0.524987   \n",
       "max        0.037676      0.017639      0.022246      0.008829      0.599638   \n",
       "\n",
       "                 15  \n",
       "count  42039.000000  \n",
       "mean       2.030716  \n",
       "std        1.827649  \n",
       "min       -1.198834  \n",
       "25%        0.466435  \n",
       "50%        2.031259  \n",
       "75%        3.600611  \n",
       "max        5.194131  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42039.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.011824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.003658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.009818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.011084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.012735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.070915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target\n",
       "count  42039.000000\n",
       "mean       0.011824\n",
       "std        0.003658\n",
       "min        0.007400\n",
       "25%        0.009818\n",
       "50%        0.011084\n",
       "75%        0.012735\n",
       "max        0.070915"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indep_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SobolevNetwork(Model):\n",
    "    def __init__(self, input_dim, num_hidden,init = None):\n",
    "        super(SobolevNetwork, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.num_hidden = num_hidden\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.input_dim, self.num_hidden],stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.dp1 = tf.keras.layers.Dropout(0.2)\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.dp2 = tf.keras.layers.Dropout(0.2)\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.dp3 = tf.keras.layers.Dropout(0.2)\n",
    "        self.W4 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b4 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.dp4 = tf.keras.layers.Dropout(0.2)\n",
    "        self.W5 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b5 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.dp5 = tf.keras.layers.Dropout(0.2)\n",
    "        self.W6 = tf.Variable(tf.random.normal([self.num_hidden, self.num_hidden],stddev=0.1))\n",
    "        self.b6 = tf.Variable(tf.ones([self.num_hidden]))\n",
    "        self.dp6 = tf.keras.layers.Dropout(0.2)        \n",
    "        self.W7 = tf.Variable(tf.random.normal([self.num_hidden, 1],stddev=0.1))\n",
    "        self.b7 = tf.Variable(tf.ones([1]))\n",
    "        self.w = [(self.W1, self.b1), (self.W2, self.b2), (self.W3, self.b3),(self.W4, self.b4), (self.W5, self.b5), (self.W6, self.b6),(self.W7, self.b7)]\n",
    "        \n",
    "    def call(self, X):\n",
    "        #Input layer\n",
    "        out = X\n",
    "        #Hidden layers\n",
    "        W,b = self.w[0]\n",
    "        out = tf.nn.tanh(tf.matmul(out, W) + b)\n",
    "        out = self.dp1(out)\n",
    "        W,b = self.w[1]\n",
    "        out = tf.nn.tanh(tf.matmul(out, W) + b)\n",
    "        out = self.dp2(out)\n",
    "        W,b = self.w[2]\n",
    "        out = tf.nn.sigmoid(tf.matmul(out, W) + b)\n",
    "        out = self.dp3(out)\n",
    "        W,b = self.w[3]\n",
    "        out = tf.nn.sigmoid(tf.matmul(out, W) + b)\n",
    "        out = self.dp4(out)\n",
    "        W,b = self.w[4]\n",
    "        out = tf.nn.leaky_relu(tf.matmul(out, W) + b)\n",
    "        out = self.dp5(out)\n",
    "        W,b = self.w[5]\n",
    "        out = tf.nn.relu(tf.matmul(out, W) + b)\n",
    "        #Output layer\n",
    "        W,b = self.w[-1]\n",
    "        out = tf.matmul(out, W) + b\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\n For Epoch Number {} the model has loss[mean_absolute_error] of {}\".format(epoch+1, logs[\"loss\"]))\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        print(\"\\n For Batch Number {} the model has loss[mean_absolute_error] of {}\".format(batch+1, logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_callback = CSVLogger( 'logs.csv',separator=',',append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping( monitor='val_loss',min_delta=0,patience=2, verbose=1, mode=\"auto\", baseline=None, restore_best_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedular(epoch, lr):\n",
    "    if epoch <= 3:\n",
    "        learning_rate = lr\n",
    "    else:\n",
    "        learning_rate = lr * tf.math.exp(-0.1)\n",
    "        learning_rate = learning_rate.numpy()\n",
    "    \n",
    "\n",
    "    # with train_writer.as_default():\n",
    "    #     tf.summary.scalar('LearningRateScheduler', data= learning_rate, step= epoch)\n",
    "    \n",
    "    return learning_rate\n",
    "\n",
    "schedular_callback = LearningRateScheduler(schedular, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_1 = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_snn= SobolevNetwork(input_dim=16,num_hidden=num_hidden_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_snn.compile(optimizer=tf.keras.optimizers.Adam(),  \n",
    "            loss='mean_absolute_error',metrics=[\"mean_squared_error\",\"mean_absolute_error\",\"mean_absolute_percentage_error\",\"mean_squared_logarithmic_error\"],run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss= StandardScaler()\n",
    "dep_train_ss= pd.DataFrame(ss.fit_transform(dep_train))\n",
    "dep_val_ss= pd.DataFrame(ss.transform(dep_val))\n",
    "dep_test_ss= pd.DataFrame(ss.transform(dep_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "      <td>4.203900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.133701e-16</td>\n",
       "      <td>1.243775e-16</td>\n",
       "      <td>-1.429037e-16</td>\n",
       "      <td>3.735868e-17</td>\n",
       "      <td>1.960552e-16</td>\n",
       "      <td>2.026126e-17</td>\n",
       "      <td>6.397139e-17</td>\n",
       "      <td>-3.126076e-16</td>\n",
       "      <td>2.550946e-17</td>\n",
       "      <td>-2.887863e-17</td>\n",
       "      <td>-1.310617e-16</td>\n",
       "      <td>9.100401e-17</td>\n",
       "      <td>-8.184788e-17</td>\n",
       "      <td>3.458306e-18</td>\n",
       "      <td>1.065876e-15</td>\n",
       "      <td>-1.889114e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "      <td>1.000012e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.739425e+00</td>\n",
       "      <td>-7.478131e+00</td>\n",
       "      <td>-5.337138e+00</td>\n",
       "      <td>-4.760275e+00</td>\n",
       "      <td>-4.091860e+00</td>\n",
       "      <td>-6.351592e+00</td>\n",
       "      <td>-4.788900e+00</td>\n",
       "      <td>-1.729144e+00</td>\n",
       "      <td>-5.607404e+00</td>\n",
       "      <td>-5.049549e+00</td>\n",
       "      <td>-6.632866e+00</td>\n",
       "      <td>-8.083431e+00</td>\n",
       "      <td>-5.676416e+00</td>\n",
       "      <td>-4.797768e+00</td>\n",
       "      <td>-1.753407e+00</td>\n",
       "      <td>-1.767072e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.597252e-01</td>\n",
       "      <td>-6.675516e-01</td>\n",
       "      <td>-6.865854e-01</td>\n",
       "      <td>-6.953154e-01</td>\n",
       "      <td>-6.685763e-01</td>\n",
       "      <td>-7.004953e-01</td>\n",
       "      <td>-6.623642e-01</td>\n",
       "      <td>-8.630342e-01</td>\n",
       "      <td>-6.171298e-01</td>\n",
       "      <td>-7.259875e-01</td>\n",
       "      <td>-6.987723e-01</td>\n",
       "      <td>-7.208950e-01</td>\n",
       "      <td>-7.202097e-01</td>\n",
       "      <td>-7.400159e-01</td>\n",
       "      <td>-8.551083e-01</td>\n",
       "      <td>-8.559080e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.834259e-03</td>\n",
       "      <td>2.375580e-02</td>\n",
       "      <td>-9.484321e-03</td>\n",
       "      <td>-3.284211e-02</td>\n",
       "      <td>-4.649756e-02</td>\n",
       "      <td>-4.942010e-02</td>\n",
       "      <td>3.059965e-02</td>\n",
       "      <td>-3.033257e-03</td>\n",
       "      <td>6.948886e-02</td>\n",
       "      <td>-1.018696e-01</td>\n",
       "      <td>-1.867750e-02</td>\n",
       "      <td>4.877867e-03</td>\n",
       "      <td>-4.388725e-02</td>\n",
       "      <td>-4.399778e-02</td>\n",
       "      <td>1.841595e-02</td>\n",
       "      <td>2.975047e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.603870e-01</td>\n",
       "      <td>6.652526e-01</td>\n",
       "      <td>6.831795e-01</td>\n",
       "      <td>6.396575e-01</td>\n",
       "      <td>5.957895e-01</td>\n",
       "      <td>6.433655e-01</td>\n",
       "      <td>6.542139e-01</td>\n",
       "      <td>8.549207e-01</td>\n",
       "      <td>6.948614e-01</td>\n",
       "      <td>6.408546e-01</td>\n",
       "      <td>6.659730e-01</td>\n",
       "      <td>7.542428e-01</td>\n",
       "      <td>6.466437e-01</td>\n",
       "      <td>7.039838e-01</td>\n",
       "      <td>8.576049e-01</td>\n",
       "      <td>8.589801e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.734967e+00</td>\n",
       "      <td>3.939765e+00</td>\n",
       "      <td>5.963242e+00</td>\n",
       "      <td>6.762325e+00</td>\n",
       "      <td>6.159264e+00</td>\n",
       "      <td>5.383953e+00</td>\n",
       "      <td>9.257489e+00</td>\n",
       "      <td>1.756538e+00</td>\n",
       "      <td>7.303540e+00</td>\n",
       "      <td>5.078544e+00</td>\n",
       "      <td>4.389104e+00</td>\n",
       "      <td>4.050774e+00</td>\n",
       "      <td>8.607465e+00</td>\n",
       "      <td>4.740577e+00</td>\n",
       "      <td>1.724766e+00</td>\n",
       "      <td>1.730886e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  4.203900e+04  4.203900e+04  4.203900e+04  4.203900e+04  4.203900e+04   \n",
       "mean   1.133701e-16  1.243775e-16 -1.429037e-16  3.735868e-17  1.960552e-16   \n",
       "std    1.000012e+00  1.000012e+00  1.000012e+00  1.000012e+00  1.000012e+00   \n",
       "min   -1.739425e+00 -7.478131e+00 -5.337138e+00 -4.760275e+00 -4.091860e+00   \n",
       "25%   -8.597252e-01 -6.675516e-01 -6.865854e-01 -6.953154e-01 -6.685763e-01   \n",
       "50%    2.834259e-03  2.375580e-02 -9.484321e-03 -3.284211e-02 -4.649756e-02   \n",
       "75%    8.603870e-01  6.652526e-01  6.831795e-01  6.396575e-01  5.957895e-01   \n",
       "max    1.734967e+00  3.939765e+00  5.963242e+00  6.762325e+00  6.159264e+00   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  4.203900e+04  4.203900e+04  4.203900e+04  4.203900e+04  4.203900e+04   \n",
       "mean   2.026126e-17  6.397139e-17 -3.126076e-16  2.550946e-17 -2.887863e-17   \n",
       "std    1.000012e+00  1.000012e+00  1.000012e+00  1.000012e+00  1.000012e+00   \n",
       "min   -6.351592e+00 -4.788900e+00 -1.729144e+00 -5.607404e+00 -5.049549e+00   \n",
       "25%   -7.004953e-01 -6.623642e-01 -8.630342e-01 -6.171298e-01 -7.259875e-01   \n",
       "50%   -4.942010e-02  3.059965e-02 -3.033257e-03  6.948886e-02 -1.018696e-01   \n",
       "75%    6.433655e-01  6.542139e-01  8.549207e-01  6.948614e-01  6.408546e-01   \n",
       "max    5.383953e+00  9.257489e+00  1.756538e+00  7.303540e+00  5.078544e+00   \n",
       "\n",
       "                 10            11            12            13            14  \\\n",
       "count  4.203900e+04  4.203900e+04  4.203900e+04  4.203900e+04  4.203900e+04   \n",
       "mean  -1.310617e-16  9.100401e-17 -8.184788e-17  3.458306e-18  1.065876e-15   \n",
       "std    1.000012e+00  1.000012e+00  1.000012e+00  1.000012e+00  1.000012e+00   \n",
       "min   -6.632866e+00 -8.083431e+00 -5.676416e+00 -4.797768e+00 -1.753407e+00   \n",
       "25%   -6.987723e-01 -7.208950e-01 -7.202097e-01 -7.400159e-01 -8.551083e-01   \n",
       "50%   -1.867750e-02  4.877867e-03 -4.388725e-02 -4.399778e-02  1.841595e-02   \n",
       "75%    6.659730e-01  7.542428e-01  6.466437e-01  7.039838e-01  8.576049e-01   \n",
       "max    4.389104e+00  4.050774e+00  8.607465e+00  4.740577e+00  1.724766e+00   \n",
       "\n",
       "                 15  \n",
       "count  4.203900e+04  \n",
       "mean  -1.889114e-15  \n",
       "std    1.000012e+00  \n",
       "min   -1.767072e+00  \n",
       "25%   -8.559080e-01  \n",
       "50%    2.975047e-04  \n",
       "75%    8.589801e-01  \n",
       "max    1.730886e+00  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_train_ss.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Calling `Model.fit` in graph mode is not supported when the `Model` instance was constructed with eager mode enabled. Please construct your `Model` instance in graph mode or call `Model.fit` with eager mode enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\bro\\Analysis\\mSANN\\cd\\test\\final.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m custom_snn\u001b[39m.\u001b[39;49mfit(dep_train, indep_train, validation_data\u001b[39m=\u001b[39;49m(dep_val,indep_val),batch_size \u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m, epochs \u001b[39m=\u001b[39;49m \u001b[39m8\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[schedular_callback,LossCallback(), csv_callback,tensorboard_callback])\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras\\utils\\version_utils.py:126\u001b[0m, in \u001b[0;36mdisallow_legacy_graph\u001b[1;34m(cls_name, method_name)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mexecuting_eagerly_outside_functions():\n\u001b[0;32m    119\u001b[0m     error_msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    120\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCalling `\u001b[39m\u001b[39m{\u001b[39;00mcls_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmethod_name\u001b[39m}\u001b[39;00m\u001b[39m` in graph mode is not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msupported when the `\u001b[39m\u001b[39m{\u001b[39;00mcls_name\u001b[39m}\u001b[39;00m\u001b[39m` instance was constructed with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39meager mode enabled.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m     )\n\u001b[1;32m--> 126\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Calling `Model.fit` in graph mode is not supported when the `Model` instance was constructed with eager mode enabled. Please construct your `Model` instance in graph mode or call `Model.fit` with eager mode enabled."
     ]
    }
   ],
   "source": [
    "custom_snn.fit(dep_train, indep_train, validation_data=(dep_val,indep_val),batch_size = 1000, epochs = 8, callbacks=[schedular_callback,LossCallback(), csv_callback,tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SobolevNetwork at 0x2b857285280>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_snn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dalex as dx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "keras is no longer supported, please use tf.keras instead.\n",
      "Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\bro\\Analysis\\mSANN\\cd\\test\\final.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mdisable_v2_behavior()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X62sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m background \u001b[39m=\u001b[39m dep_train\u001b[39m.\u001b[39miloc[np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(dep_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m100\u001b[39m, replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X62sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m explainer \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39;49mDeepExplainer(custom_snn, dep_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X62sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m shap_values \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39mshap_values(dep_test)\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\__init__.py:84\u001b[0m, in \u001b[0;36mDeep.__init__\u001b[1;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[0;32m     81\u001b[0m         framework \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[39mif\u001b[39;00m framework \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtensorflow\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplainer \u001b[39m=\u001b[39m TFDeep(model, data, session, learning_phase_flags)\n\u001b[0;32m     85\u001b[0m \u001b[39melif\u001b[39;00m framework \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpytorch\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     86\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplainer \u001b[39m=\u001b[39m PyTorchDeep(model, data)\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:104\u001b[0m, in \u001b[0;36mTFDeep.__init__\u001b[1;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39m# determine the model inputs and outputs\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_inputs \u001b[39m=\u001b[39m _get_model_inputs(model)\n\u001b[1;32m--> 104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_output \u001b[39m=\u001b[39m _get_model_output(model)\n\u001b[0;32m    105\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_output) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mThe model output to be explained must be a single tensor!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_output\u001b[39m.\u001b[39mshape) \u001b[39m<\u001b[39m \u001b[39m3\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mThe model output must be a vector or a single value!\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\shap\\explainers\\tf_utils.py:85\u001b[0m, in \u001b[0;36m_get_model_output\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(model))\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39mkeras.engine.sequential.Sequential\u001b[39m\u001b[39m'\u001b[39m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \\\n\u001b[0;32m     81\u001b[0m     \u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(model))\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39mkeras.models.Sequential\u001b[39m\u001b[39m'\u001b[39m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \\\n\u001b[0;32m     82\u001b[0m     \u001b[39mstr\u001b[39m(\u001b[39mtype\u001b[39m(model))\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39mkeras.engine.training.Model\u001b[39m\u001b[39m'\u001b[39m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \\\n\u001b[0;32m     83\u001b[0m     \u001b[39misinstance\u001b[39m(model, tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel):\n\u001b[0;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39m_inbound_nodes) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 85\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(model\u001b[39m.\u001b[39;49moutputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     86\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mOnly one model output supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m         \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39moutputs[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# compute SHAP values\n",
    "import shap\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "background = dep_train.iloc[np.random.choice(dep_train.shape[0], 100, replace=False)]\n",
    "explainer = shap.DeepExplainer(custom_snn, dep_train)\n",
    "shap_values = explainer.shap_values(dep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[0], plot_type = 'bar', feature_names = test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target\n",
       "0  0.016498\n",
       "1  0.014071\n",
       "2  0.016428\n",
       "3  0.012589\n",
       "4  0.013429"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indep_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\bro\\Analysis\\mSANN\\cd\\test\\final.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m custom_snn\u001b[39m.\u001b[39;49mevaluate(dep_test, indep_test)\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "custom_snn.evaluate(dep_test, indep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 2s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.01232105],\n",
       "       [0.01232111],\n",
       "       [0.01232111],\n",
       "       ...,\n",
       "       [0.01232111],\n",
       "       [0.01232111],\n",
       "       [0.01232105]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_snn.predict(dep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x0000023181EF3EE0>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: final1\\assets\n"
     ]
    }
   ],
   "source": [
    "custom_snn.save(\"final1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0006703201215714216.\n",
      "Epoch 1/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.003126361407339573}\n",
      " 1/43 [..............................] - ETA: 28s - loss: 0.0031\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.00313993520103395}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.0030545908957719803}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.002987976185977459}\n",
      " 4/43 [=>............................] - ETA: 0s - loss: 0.0030 \n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0030163717456161976}\n",
      "\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0030706014949828386}\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0151s vs `on_train_batch_end` time: 0.1110s). Check your callbacks.\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.003112067934125662}\n",
      " 7/43 [===>..........................] - ETA: 0s - loss: 0.0031\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.003116022562608123}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0030998005531728268}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.003138576168566942}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.003147140610963106}\n",
      "11/43 [======>.......................] - ETA: 0s - loss: 0.0031\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0031267160084098577}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.0031253923662006855}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.003130133030936122}\n",
      "14/43 [========>.....................] - ETA: 0s - loss: 0.0031\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.0030915727838873863}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.003090325277298689}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.0030840281397104263}\n",
      "17/43 [==========>...................] - ETA: 0s - loss: 0.0031\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0030760413501411676}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0030720962677150965}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.0030760690569877625}\n",
      "20/43 [============>.................] - ETA: 0s - loss: 0.0031\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.003071043873205781}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0030740879010409117}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.003082436276599765}\n",
      "23/43 [===============>..............] - ETA: 0s - loss: 0.0031\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0030756189953535795}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0030710918363183737}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.003074084408581257}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.003072265535593033}\n",
      "27/43 [=================>............] - ETA: 0s - loss: 0.0031\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0030839024111628532}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.00307458802126348}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.003069186583161354}\n",
      "30/43 [===================>..........] - ETA: 0s - loss: 0.0031\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.0030610065441578627}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.003071051323786378}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.003068012883886695}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0030740404035896063}\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0031\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.0030616167932748795}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.003049905877560377}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0030476707033813}\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0030\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.0030432373750954866}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.0030414932407438755}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.003035350237041712}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.003034624271094799}\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0030\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.003040330484509468}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.0030442194547504187}\n",
      "\n",
      " For Epoch Number 1 the model has loss[mean_absolute_error] of 0.0030442194547504187\n",
      "43/43 [==============================] - 2s 23ms/step - loss: 0.0030 - val_loss: 0.0021 - lr: 6.7032e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0006703201215714216.\n",
      "Epoch 2/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0026943383272737265}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0028606397099792957}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.002864798530936241}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.002896450227126479}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.002871382050216198}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0029\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.002889798255637288}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0029096638318151236}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0029241000302135944}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0029568769969046116}\n",
      " 9/43 [=====>........................] - ETA: 0s - loss: 0.0030\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.0029393373988568783}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0029420782811939716}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.002945664804428816}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.002919517457485199}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.0028765015304088593}\n",
      "14/43 [========>.....................] - ETA: 0s - loss: 0.0029\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.002868564333766699}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.00287740514613688}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.002860135631635785}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.002874017460271716}\n",
      "18/43 [===========>..................] - ETA: 0s - loss: 0.0029\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.002858000574633479}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.0028457208536565304}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0028414481785148382}\n",
      "21/43 [=============>................] - ETA: 0s - loss: 0.0028\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0028445315547287464}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0028543509542942047}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0028455632273107767}\n",
      "24/43 [===============>..............] - ETA: 0s - loss: 0.0028\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0028526117093861103}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.002846461720764637}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.002848622389137745}\n",
      "27/43 [=================>............] - ETA: 0s - loss: 0.0028\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.002845630282536149}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.0028541283681988716}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0028494945727288723}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.00284843728877604}\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 0.0028\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.0028675138019025326}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0028596529737114906}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0028636078350245953}\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0029\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.0028725548181682825}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.0028758698608726263}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.002891474636271596}\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0029\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.0028807888738811016}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.002880195388570428}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.002873094519600272}\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0029\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0028706605080515146}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.00286968145519495}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.002869904274120927}\n",
      "\n",
      " For Epoch Number 2 the model has loss[mean_absolute_error] of 0.002869904274120927\n",
      "43/43 [==============================] - 1s 19ms/step - loss: 0.0029 - val_loss: 0.0021 - lr: 6.7032e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0006703201215714216.\n",
      "Epoch 3/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.002869930351153016}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0029\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.002726558595895767}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.002812249818816781}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.002787702949717641}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0027988676447421312}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0028\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0027510058134794235}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.002717994386330247}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.002717458177357912}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.002716178074479103}\n",
      " 9/43 [=====>........................] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.0027048003394156694}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0027021223213523626}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0027015411760658026}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.002696956042200327}\n",
      "13/43 [========>.....................] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.00270459009334445}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.0027174213901162148}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.002732872497290373}\n",
      "16/43 [==========>...................] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.002739954274147749}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.002740689553320408}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0027293390594422817}\n",
      "19/43 [============>.................] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.0027158723678439856}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0027042271103709936}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.002708306536078453}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0027121836319565773}\n",
      "23/43 [===============>..............] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.002706252969801426}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.002699831034988165}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0027003949508070946}\n",
      "26/43 [=================>............] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.0027062695007771254}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0026915890630334616}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.0026921869721263647}\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.002695690607652068}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.0027024962473660707}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.0027131482493132353}\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0027088383212685585}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0027172481641173363}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.002717886120080948}\n",
      "35/43 [=======================>......] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.0027088781353086233}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0027106148190796375}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.0027091677766293287}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.002704304875805974}\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0026995320804417133}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0026986226439476013}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0026989595498889685}\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.0026992089115083218}\n",
      "\n",
      " For Epoch Number 3 the model has loss[mean_absolute_error] of 0.0026992089115083218\n",
      "43/43 [==============================] - 1s 20ms/step - loss: 0.0027 - val_loss: 0.0021 - lr: 6.7032e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0006703201215714216.\n",
      "Epoch 4/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.002678095595911145}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0026414005551487207}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.0026364668738096952}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.002606143243610859}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0027087628841400146}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0027002294082194567}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0027143172919750214}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0026820802595466375}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0026615357492119074}\n",
      " 9/43 [=====>........................] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.0026768785901367664}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0026522865518927574}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0026703558396548033}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.002674973336979747}\n",
      "13/43 [========>.....................] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.0026812732685357332}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.0026653818786144257}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.002689105225726962}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.002713522408157587}\n",
      "17/43 [==========>...................] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0027135745622217655}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0027106902562081814}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.002687969245016575}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0026722881011664867}\n",
      "21/43 [=============>................] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.002676294418051839}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0026651713997125626}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0026610970962792635}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.002647627843543887}\n",
      "25/43 [================>.............] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.002642678562551737}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.00264574377797544}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.002649750094860792}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.002645091386511922}\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0026404387317597866}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.002631697803735733}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.002619439270347357}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0026159114204347134}\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.002620050450786948}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.002617419697344303}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.002622125670313835}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0026218981947749853}\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.0026223589666187763}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.002627866342663765}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.002627663081511855}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0026231573428958654}\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0026254814583808184}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.0026250421069562435}\n",
      "\n",
      " For Epoch Number 4 the model has loss[mean_absolute_error] of 0.0026250421069562435\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0026 - val_loss: 0.0022 - lr: 6.7032e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0006065307534299791.\n",
      "Epoch 5/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.00263606128282845}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0026427912525832653}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.00264354282990098}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.002598210470750928}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0025794710963964462}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0025874993298202753}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.002565192524343729}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0025667741429060698}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.002548977266997099}\n",
      " 9/43 [=====>........................] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.002553457859903574}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0025535032618790865}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0025509914848953485}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.002577252686023712}\n",
      "13/43 [========>.....................] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.0025788310449570417}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.002575452206656337}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0025653927586972713}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.002576458500698209}\n",
      "17/43 [==========>...................] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.002565742237493396}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.002573021687567234}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.002569998614490032}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0025664016138762236}\n",
      "21/43 [=============>................] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0025566918775439262}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0025443085469305515}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0025339932180941105}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0025387222412973642}\n",
      "25/43 [================>.............] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0025354099925607443}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.0025369315408170223}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0025305242743343115}\n",
      "28/43 [==================>...........] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.002536253072321415}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.002542438916862011}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.00254345522262156}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.0025431171525269747}\n",
      "32/43 [=====================>........] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0025459949392825365}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.002551184268668294}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.0025511709973216057}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.0025491579435765743}\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.0025454566348344088}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.002549999626353383}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.002549913711845875}\n",
      "39/43 [==========================>...] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0025439970195293427}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0025455565191805363}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.002538166241720319}\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.0025386286433786154}\n",
      "\n",
      " For Epoch Number 5 the model has loss[mean_absolute_error] of 0.0025386286433786154\n",
      "43/43 [==============================] - 1s 19ms/step - loss: 0.0025 - val_loss: 0.0026 - lr: 6.0653e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0005488117458298802.\n",
      "Epoch 6/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0030778974760323763}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0031\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.002844668924808502}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.0027809718158096075}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.002686572726815939}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0027328100986778736}\n",
      "\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.002749253297224641}\n",
      " 6/43 [===>..........................] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0026948791928589344}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.0026740077883005142}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0026517920196056366}\n",
      "\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.0026857464108616114}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0026668694335967302}\n",
      "11/43 [======>.......................] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.002648133086040616}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.0026384794618934393}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.002618605038151145}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.002609459450468421}\n",
      "15/43 [=========>....................] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0026110357139259577}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.0026004274841398}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.002593534765765071}\n",
      "18/43 [===========>..................] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0025782748125493526}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.0025680416729301214}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0025632171891629696}\n",
      "21/43 [=============>................] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.00255792960524559}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.002542120637372136}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.002546150702983141}\n",
      "24/43 [===============>..............] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.002550313016399741}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0025484131183475256}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.0025515705347061157}\n",
      "27/43 [=================>............] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0025501959025859833}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.002536976244300604}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.002531520090997219}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.002526858588680625}\n",
      "31/43 [====================>.........] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.002515242202207446}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0025098351761698723}\n",
      "\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0025040237233042717}\n",
      "34/43 [======================>.......] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.002511854749172926}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.002508691046386957}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.002505288925021887}\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.0025029308162629604}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.0025026341900229454}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.002504257718101144}\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.002507658675312996}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.00250703445635736}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.002506403950974345}\n",
      "43/43 [==============================] - ETA: 0s - loss: 0.0025\n",
      " For Epoch Number 6 the model has loss[mean_absolute_error] of 0.002506403950974345\n",
      "43/43 [==============================] - 1s 22ms/step - loss: 0.0025 - val_loss: 0.0029 - lr: 5.4881e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0004965853877365589.\n",
      "Epoch 7/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.0031175303738564253}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0031\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.0028722919523715973}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.0028588040731847286}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.002740587340667844}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.002719253534451127}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0027\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0026682568714022636}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0026493363548070192}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.00260168663226068}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.0026167789474129677}\n",
      " 9/43 [=====>........................] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.002587059047073126}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.002575884573161602}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.002587168477475643}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.00259400624781847}\n",
      "\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.002583129797130823}\n",
      "14/43 [========>.....................] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.0025850993115454912}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0025686121080070734}\n",
      "\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.002552806166931987}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0025631955359131098}\n",
      "18/43 [===========>..................] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.002556672552600503}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.0025422675535082817}\n",
      "\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0025509167462587357}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0025477230083197355}\n",
      "22/43 [==============>...............] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0025380474980920553}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0025384468026459217}\n",
      "\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.0025391161907464266}\n",
      "25/43 [================>.............] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.0025321035645902157}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.002521910471841693}\n",
      "\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.002504928270354867}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.00250818720087409}\n",
      "29/43 [===================>..........] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0025157376658171415}\n",
      "\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.0025088253896683455}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.002512849634513259}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.002509943675249815}\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.002505280775949359}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.0025001808535307646}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.0024923866149038076}\n",
      "\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.002483951160684228}\n",
      "37/43 [========================>.....] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.0024850890040397644}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.0024819374084472656}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.0024810913018882275}\n",
      "\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0024763564579188824}\n",
      "41/43 [===========================>..] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.0024720721412450075}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.002472064457833767}\n",
      "\n",
      " For Epoch Number 7 the model has loss[mean_absolute_error] of 0.002472064457833767\n",
      "43/43 [==============================] - 1s 19ms/step - loss: 0.0025 - val_loss: 0.0022 - lr: 4.9659e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0004493290325626731.\n",
      "Epoch 8/8\n",
      "\n",
      " For Batch Number 1 the model has loss[mean_absolute_error] of {'loss': 0.002565167611464858}\n",
      " 1/43 [..............................] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 2 the model has loss[mean_absolute_error] of {'loss': 0.002516621258109808}\n",
      "\n",
      " For Batch Number 3 the model has loss[mean_absolute_error] of {'loss': 0.0024967771023511887}\n",
      "\n",
      " For Batch Number 4 the model has loss[mean_absolute_error] of {'loss': 0.0025512068532407284}\n",
      "\n",
      " For Batch Number 5 the model has loss[mean_absolute_error] of {'loss': 0.0026132750790566206}\n",
      " 5/43 [==>...........................] - ETA: 0s - loss: 0.0026\n",
      " For Batch Number 6 the model has loss[mean_absolute_error] of {'loss': 0.0025861221365630627}\n",
      "\n",
      " For Batch Number 7 the model has loss[mean_absolute_error] of {'loss': 0.0025739152915775776}\n",
      "\n",
      " For Batch Number 8 the model has loss[mean_absolute_error] of {'loss': 0.002543454524129629}\n",
      "\n",
      " For Batch Number 9 the model has loss[mean_absolute_error] of {'loss': 0.002528153359889984}\n",
      " 9/43 [=====>........................] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 10 the model has loss[mean_absolute_error] of {'loss': 0.0025210471358150244}\n",
      "\n",
      " For Batch Number 11 the model has loss[mean_absolute_error] of {'loss': 0.0025036614388227463}\n",
      "\n",
      " For Batch Number 12 the model has loss[mean_absolute_error] of {'loss': 0.0025017112493515015}\n",
      "\n",
      " For Batch Number 13 the model has loss[mean_absolute_error] of {'loss': 0.0024766437709331512}\n",
      "13/43 [========>.....................] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 14 the model has loss[mean_absolute_error] of {'loss': 0.002478491747751832}\n",
      "\n",
      " For Batch Number 15 the model has loss[mean_absolute_error] of {'loss': 0.002476360648870468}\n",
      "\n",
      " For Batch Number 16 the model has loss[mean_absolute_error] of {'loss': 0.0024594427086412907}\n",
      "16/43 [==========>...................] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 17 the model has loss[mean_absolute_error] of {'loss': 0.002456096699461341}\n",
      "\n",
      " For Batch Number 18 the model has loss[mean_absolute_error] of {'loss': 0.0024445438757538795}\n",
      "\n",
      " For Batch Number 19 the model has loss[mean_absolute_error] of {'loss': 0.0024513991083949804}\n",
      "\n",
      " For Batch Number 20 the model has loss[mean_absolute_error] of {'loss': 0.0024590115062892437}\n",
      "20/43 [============>.................] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 21 the model has loss[mean_absolute_error] of {'loss': 0.0024704094976186752}\n",
      "\n",
      " For Batch Number 22 the model has loss[mean_absolute_error] of {'loss': 0.0024666355457156897}\n",
      "\n",
      " For Batch Number 23 the model has loss[mean_absolute_error] of {'loss': 0.0024688749108463526}\n",
      "\n",
      " For Batch Number 24 the model has loss[mean_absolute_error] of {'loss': 0.0024559386074543}\n",
      "24/43 [===============>..............] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 25 the model has loss[mean_absolute_error] of {'loss': 0.00244710361585021}\n",
      "\n",
      " For Batch Number 26 the model has loss[mean_absolute_error] of {'loss': 0.002443538745865226}\n",
      "\n",
      " For Batch Number 27 the model has loss[mean_absolute_error] of {'loss': 0.002439153380692005}\n",
      "27/43 [=================>............] - ETA: 0s - loss: 0.0024\n",
      " For Batch Number 28 the model has loss[mean_absolute_error] of {'loss': 0.0024377184454351664}\n",
      "\n",
      " For Batch Number 29 the model has loss[mean_absolute_error] of {'loss': 0.002439074916765094}\n",
      "\n",
      " For Batch Number 30 the model has loss[mean_absolute_error] of {'loss': 0.0024378017988055944}\n",
      "30/43 [===================>..........] - ETA: 0s - loss: 0.0024\n",
      " For Batch Number 31 the model has loss[mean_absolute_error] of {'loss': 0.0024401287082582712}\n",
      "\n",
      " For Batch Number 32 the model has loss[mean_absolute_error] of {'loss': 0.002447158098220825}\n",
      "\n",
      " For Batch Number 33 the model has loss[mean_absolute_error] of {'loss': 0.0024429087061434984}\n",
      "33/43 [======================>.......] - ETA: 0s - loss: 0.0024\n",
      " For Batch Number 34 the model has loss[mean_absolute_error] of {'loss': 0.0024516426492482424}\n",
      "\n",
      " For Batch Number 35 the model has loss[mean_absolute_error] of {'loss': 0.002450833562761545}\n",
      "\n",
      " For Batch Number 36 the model has loss[mean_absolute_error] of {'loss': 0.0024560182355344296}\n",
      "36/43 [========================>.....] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 37 the model has loss[mean_absolute_error] of {'loss': 0.002455259906128049}\n",
      "\n",
      " For Batch Number 38 the model has loss[mean_absolute_error] of {'loss': 0.002449193736538291}\n",
      "\n",
      " For Batch Number 39 the model has loss[mean_absolute_error] of {'loss': 0.002455580048263073}\n",
      "\n",
      " For Batch Number 40 the model has loss[mean_absolute_error] of {'loss': 0.002459364477545023}\n",
      "40/43 [==========================>...] - ETA: 0s - loss: 0.0025\n",
      " For Batch Number 41 the model has loss[mean_absolute_error] of {'loss': 0.0024563707411289215}\n",
      "\n",
      " For Batch Number 42 the model has loss[mean_absolute_error] of {'loss': 0.002458056900650263}\n",
      "\n",
      " For Batch Number 43 the model has loss[mean_absolute_error] of {'loss': 0.0024575882125645876}\n",
      "\n",
      " For Epoch Number 8 the model has loss[mean_absolute_error] of 0.0024575882125645876\n",
      "43/43 [==============================] - 1s 20ms/step - loss: 0.0025 - val_loss: 0.0025 - lr: 4.4933e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x231855c0610>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_snn.fit(dep_train_ss, indep_train, validation_data=(dep_val_ss,indep_val),batch_size = 1000, epochs = 8, callbacks=[schedular_callback,LossCallback(), csv_callback,tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_snn.predict(dep_test_ss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 1s 3ms/step - loss: 0.0025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0024894324596971273"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_snn.evaluate(dep_test_ss, indep_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17416), started 5 days, 1:03:26 ago. (Use '!kill 17416' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9362ecab5426915c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9362ecab5426915c\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  # model = tf.keras.Sequential()\n",
    "  # model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "  # hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
    "  hp_layer_1 = hp.Int('layer_1', min_value=1, max_value=1000, step=100)\n",
    "  # hp_layer_2 = hp.Int('layer_2', min_value=1, max_value=1000, step=100)\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "#   model.add(tf.keras.layers.Dense(units=hp_layer_1, activation=hp_activation))\n",
    "#   model.add(tf.keras.layers.Dense(units=hp_layer_2, activation=hp_activation))\n",
    "#   model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "  model= SobolevNetwork(input_dim=16,num_hidden=hp_layer_1)\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='mean_squared_error', metrics=[\"mean_squared_error\"])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project dir\\x\\oracle.json\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     \n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='dir',\n",
    "                     project_name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "901               |?                 |layer_1\n",
      "0.01              |?                 |learning_rate\n",
      "2                 |?                 |tuner/epochs\n",
      "0                 |?                 |tuner/initial_epoch\n",
      "2                 |?                 |tuner/bracket\n",
      "0                 |?                 |tuner/round\n",
      "\n",
      "Epoch 1/2\n",
      "1051/1051 [==============================] - 90s 83ms/step - loss: 1.5470 - val_loss: 0.0431\n",
      "Epoch 2/2\n",
      "1051/1051 [==============================] - 93s 89ms/step - loss: 0.0095 - val_loss: 0.0036\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\bro\\Analysis\\mSANN\\cd\\test\\final.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tuner\u001b[39m.\u001b[39;49msearch(dep_train, indep_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[es_callback])\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:203\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m         tuner_utils\u001b[39m.\u001b[39mvalidate_trial_results(\n\u001b[0;32m    197\u001b[0m             results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective, \u001b[39m\"\u001b[39m\u001b[39mTuner.run_trial()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         ),\n\u001b[0;32m    199\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mupdate_trial(\n\u001b[0;32m    200\u001b[0m             trial\u001b[39m.\u001b[39mtrial_id,\n\u001b[0;32m    201\u001b[0m             \u001b[39m# Convert to dictionary before calling `update_trial()`\u001b[39;00m\n\u001b[0;32m    202\u001b[0m             \u001b[39m# to pass it from gRPC.\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m             tuner_utils\u001b[39m.\u001b[39;49mconvert_to_metrics_dict(\n\u001b[0;32m    204\u001b[0m                 results,\n\u001b[0;32m    205\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moracle\u001b[39m.\u001b[39;49mobjective,\n\u001b[0;32m    206\u001b[0m             ),\n\u001b[0;32m    207\u001b[0m             step\u001b[39m=\u001b[39mtuner_utils\u001b[39m.\u001b[39mget_best_step(results, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mobjective),\n\u001b[0;32m    208\u001b[0m         )\n\u001b[0;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner_utils.py:266\u001b[0m, in \u001b[0;36mconvert_to_metrics_dict\u001b[1;34m(results, objective)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39m# List of multiple exectuion results to be averaged.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m# Check this case first to deal each case individually to check for errors.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m average_metrics_dicts(\n\u001b[1;32m--> 266\u001b[0m         [convert_to_metrics_dict(elem, objective) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m results]\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    269\u001b[0m \u001b[39m# Single value.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results, (\u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m, np\u001b[39m.\u001b[39mfloating)):\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner_utils.py:266\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39m# List of multiple exectuion results to be averaged.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39m# Check this case first to deal each case individually to check for errors.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m average_metrics_dicts(\n\u001b[1;32m--> 266\u001b[0m         [convert_to_metrics_dict(elem, objective) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m results]\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    269\u001b[0m \u001b[39m# Single value.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results, (\u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m, np\u001b[39m.\u001b[39mfloating)):\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner_utils.py:279\u001b[0m, in \u001b[0;36mconvert_to_metrics_dict\u001b[1;34m(results, objective)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39m# A History.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results, keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mHistory):\n\u001b[1;32m--> 279\u001b[0m     best_value, _ \u001b[39m=\u001b[39m _get_best_value_and_best_epoch_from_history(\n\u001b[0;32m    280\u001b[0m         results, objective\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m best_value\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner_utils.py:250\u001b[0m, in \u001b[0;36m_get_best_value_and_best_epoch_from_history\u001b[1;34m(history, objective)\u001b[0m\n\u001b[0;32m    248\u001b[0m best_epoch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    249\u001b[0m \u001b[39mfor\u001b[39;00m epoch, metrics \u001b[39min\u001b[39;00m epoch_metrics\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 250\u001b[0m     objective_value \u001b[39m=\u001b[39m objective\u001b[39m.\u001b[39;49mget_value(metrics)\n\u001b[0;32m    251\u001b[0m     \u001b[39m# Support multi-objective.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m objective\u001b[39m.\u001b[39mname \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m metrics:\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\objective.py:55\u001b[0m, in \u001b[0;36mObjective.get_value\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_value\u001b[39m(\u001b[39mself\u001b[39m, logs):\n\u001b[0;32m     45\u001b[0m     \u001b[39m\"\"\"Get the objective value from the metrics logs.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39m        The objective value.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m logs[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mse'"
     ]
    }
   ],
   "source": [
    "tuner.search(dep_train, indep_train, epochs=50, validation_split=0.2, callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57026/57026 [==============================] - 0s 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((404, 13), (102, 13), (404,), (102,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "(X_train_reg, Y_train_reg), (X_test_reg, Y_test_reg) = datasets.boston_housing.load_data()\n",
    "\n",
    "X_train_reg.shape, X_test_reg.shape, Y_train_reg.shape, Y_test_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "def build_model(hyperparams):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(X_train_reg.shape[1],)))\n",
    "    model.add(layers.Dense(units=hyperparams.Int(\"units_l1\", 16, 50, step=16),\n",
    "                           use_bias=hyperparams.Boolean(\"bias_l1\"),\n",
    "                           activation=hyperparams.Choice(\"act_l1\", [\"relu\", \"tanh\"])\n",
    "                          ))\n",
    "    model.add(layers.Dense(units=hyperparams.Int(\"units_l2\", 16, 50, step=16),\n",
    "                           use_bias=hyperparams.Boolean(\"bias_l2\"),\n",
    "                           activation=hyperparams.Choice(\"act_l2\", [\"relu\", \"tanh\"])\n",
    "                          ))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    optim=hyperparams.Choice(\"optimizer\",[\"sgd\",\"rmsprop\",\"adam\"])\n",
    "    model.compile(optim, loss=\"mean_squared_error\", metrics=[\"mean_squared_error\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 03m 03s]\n",
      "val_mean_squared_error: 0.010796456597745419\n",
      "\n",
      "Best val_mean_squared_error So Far: 0.00012853762018494308\n",
      "Total elapsed time: 00h 12m 04s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "from keras_tuner import RandomSearch\n",
    "from keras_tuner import Objective\n",
    "\n",
    "tuner1 =  RandomSearch(hypermodel=model_builder,\n",
    "                      objective=\"val_mean_squared_error\",\n",
    "                      #objective=Objective(name=\"val_mean_squared_error\",direction=\"min\"),\n",
    "                      max_trials=5,\n",
    "                      #seed=123,\n",
    "                      project_name=\"Regression2\",\n",
    "                      overwrite=True\n",
    "                    )\n",
    "\n",
    "# tuner1.search(X_train_reg, Y_train_reg, batch_size=32, epochs=10, validation_data=(X_test_reg, Y_test_reg))\n",
    "tuner1.search(dep_train, indep_train, batch_size=32, epochs=3, validation_data=(dep_val, indep_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_1': 101, 'learning_rate': 0.01}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = tuner1.get_best_hyperparameters()\n",
    "\n",
    "best_params[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                448       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,537\n",
      "Trainable params: 1,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner1.get_best_models()[0]\n",
    "\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERBAND_MAX_EPOCHS = 150\n",
    "EXECUTION_PER_TRIAL = 2\n",
    "from keras_tuner.tuners import RandomSearch, Hyperband\n",
    "tuner= Hyperband(build_model,\n",
    "                   objective= 'val_mean_squared_error',\n",
    "                   max_epochs=HYPERBAND_MAX_EPOCHS, #Set 100+ for good results\n",
    "                   executions_per_trial=EXECUTION_PER_TRIAL,\n",
    "                   directory= 'hyperband',\n",
    "                   project_name='houseprices',\n",
    "                   overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras_tuner\\engine\\metrics_tracking.py:105: RuntimeWarning: All-NaN axis encountered\n",
      "  return np.nanmin(values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0008s vs `on_train_batch_begin` time: 0.0015s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0014s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0013s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0013s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0005s vs `on_train_batch_end` time: 0.0015s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0013s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0012s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0004s). Check your callbacks.\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tuner.search(x= X_train_reg,\n",
    "             y= Y_train_reg,\n",
    "             epochs=100,\n",
    "             batch_size= 64,\n",
    "             validation_data= (X_test_reg, Y_test_reg),\n",
    "             verbose=0,\n",
    "             \n",
    "            )\n",
    "# tuner1.search(X_train_reg, Y_train_reg, batch_size=32, epochs=10, validation_data=(X_test_reg, Y_test_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras_tuner.engine.hyperparameters.HyperParameters at 0x135bb288820>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.momentum\n"
     ]
    }
   ],
   "source": [
    "# Retreive the optimal hyperparameters\n",
    "best_hps= tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the \n",
      "first densely-connected layer is 32,\n",
      "second layer is 32 \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Creating variables on a non-first call to a function decorated with tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\bro\\Analysis\\mSANN\\cd\\test\\final.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Evaluate the best model.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(best_model\u001b[39m.\u001b[39mmetrics_names)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss, mae, mse \u001b[39m=\u001b[39m best_model\u001b[39m.\u001b[39;49mevaluate(X_test_reg, Y_test_reg)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/bro/Analysis/mSANN/cd/test/final.ipynb#X42sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss:\u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m mae: \u001b[39m\u001b[39m{\u001b[39;00mmae\u001b[39m}\u001b[39;00m\u001b[39m mse: \u001b[39m\u001b[39m{\u001b[39;00mmse\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:956\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    954\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m--> 956\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    958\u001b[0m   \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m    960\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m   \u001b[39m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Creating variables on a non-first call to a function decorated with tf.function."
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the \n",
    "first densely-connected layer is {best_hps.get('units_l1')},\n",
    "second layer is {best_hps.get('units_l2')} \n",
    "  \n",
    "\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Evaluate the best model.\n",
    "print(best_model.metrics_names)\n",
    "loss, mae, mse = best_model.evaluate(X_test_reg, Y_test_reg)\n",
    "print(f'loss:{loss} mae: {mae} mse: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13bec6b3e23dd0921d86a1ec88a5a342f8b423eb92f661fc2c47e8b6a92d2fe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
