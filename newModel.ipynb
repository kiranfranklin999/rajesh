{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import random,os,io\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint,CSVLogger\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42039, 16), (42039, 1), (10935, 16), (10935, 1), (10934, 16), (10934, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dep_train=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XTr.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_train=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yTr.dat\",sep='\\s+',names=['target'])\n",
    "dep_val=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XV.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_val=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yV.dat\",sep='\\s+',names=[\"target\"])\n",
    "dep_test=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\XT.dat\",sep='\\s+',names=[str(i) for i in range(0,16)])\n",
    "indep_test=pd.read_csv(\"D:\\\\bro\\\\Analysis\\\\mSANN\\\\cd\\\\yT.dat\",sep='\\s+',names=[\"target\"])\n",
    "\n",
    "dep_train.shape,indep_train.shape,dep_val.shape,indep_val.shape,dep_test.shape,indep_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "def build_model(hyperparams):\n",
    "    hp_layer_1 = hp.Int('layer_1', min_value=1, max_value=1000, step=50)\n",
    "    hp_layer_2 = hp.Int('layer_2', min_value=1, max_value=1000, step=50)\n",
    "    hp_layer_3 = hp.Int('layer_3', min_value=1, max_value=1000, step=50)\n",
    "    hp_layer_4 = hp.Int('layer_4', min_value=1, max_value=1000, step=50)\n",
    "    hp_layer_5 = hp.Int('layer_5', min_value=1, max_value=1000, step=50)\n",
    "    hp_layer_6 = hp.Int('layer_6', min_value=1, max_value=1000, step=50)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    hp_drop_out = hp.Choice('drop_out', values=[0.1, 0.2, 0.15])\n",
    "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
    "    hp_weights=['glorot_uniform','he_normal','he_uniform','random_uniform','random_normal']\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(dep_train.shape[1],)))\n",
    "    model.add(layers.Dense(units=hp_layer_1, activation=hp_activation,kernel_initializer=hp_weights))\n",
    "    model.add(layers.Dropout(hp_drop_out))\n",
    "    model.add(layers.Dense(1,activation='linear'))\n",
    "\n",
    "    optim=hyperparams.Choice(\"optimizer\",[\"sgd\",\"rmsprop\",\"adam\"])\n",
    "    model.compile(optim, loss=\"mean_squared_error\", metrics=[\"mean_squared_error\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "model = Sequential()\n",
    "model.add(layers.Input(shape=(dep_train.shape[1],)))\n",
    "model.add(layers.Dense(units=12, activation='relu',kernel_initializer='random_normal'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(units=12, activation='linear',kernel_initializer='random_normal'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def nn_cl_bo2(neurons, activation, optimizer, learning_rate, batch_size, epochs,\n",
    "              layers1, layers2, normalization, dropout, dropout_rate):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "        \n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,]\n",
    "        \n",
    "    neurons = round(neurons)\n",
    "    activation = activationL[round(activation)]\n",
    "    optimizer = optimizerD[optimizerL[round(optimizer)]]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    layers1 = round(layers1)\n",
    "    layers2 = round(layers2)\n",
    "\n",
    "    def nn_cl_fun():\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(neurons, input_dim=10, activation=activation))\n",
    "        if normalization > 0.5:\n",
    "            nn.add(BatchNormalization())\n",
    "        for i in range(layers1):\n",
    "            nn.add(Dense(neurons, activation=activation))\n",
    "        if dropout > 0.5:\n",
    "            nn.add(Dropout(dropout_rate, seed=123))\n",
    "        for i in range(layers2):\n",
    "            nn.add(Dense(neurons, activation=activation))\n",
    "        nn.add(Dense(1, activation='linear'))\n",
    "        nn.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[\"mean_squared_error\",\"mean_absolute_error\",\"mean_absolute_percentage_error\",\"mean_squared_logarithmic_error\"])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=0, patience=20)\n",
    "    nn = KerasRegressor(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, dep_train, indep_train, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_nn2 ={\n",
    "    'neurons': (10, 100),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(20, 200),\n",
    "    'epochs':(20, 100),\n",
    "    'layers1':(1,3),\n",
    "    'layers2':(1,3),\n",
    "    'normalization':(0,1),\n",
    "    'dropout':(0,1),\n",
    "    'dropout_rate':(0,0.3)\n",
    "}\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=111)\n",
    "nn_bo.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_nn_ = nn_bo.max['params']\n",
    "\n",
    "learning_rate = params_nn_['learning_rate']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
    "\n",
    "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
    "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
    "params_nn_['layers1'] = round(params_nn_['layers1'])\n",
    "params_nn_['layers2'] = round(params_nn_['layers2'])\n",
    "params_nn_['neurons'] = round(params_nn_['neurons'])\n",
    "\n",
    "optimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','Adam']\n",
    "optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "             'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "             'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "             'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "params_nn_['optimizer'] = optimizerD[optimizerL[round(params_nn_['optimizer'])]]\n",
    "\n",
    "params_nn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    \n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                'elu', 'exponential', LeakyReLU,]\n",
    "    \n",
    "neurons = round((10, 100))\n",
    "activation = activationL[round(activation)]\n",
    "optimizer = optimizerD[optimizerL[round(optimizer)]]\n",
    "batch_size = round(batch_size)\n",
    "epochs = round(epochs)\n",
    "layers1 = round(layers1)\n",
    "layers2 = round(layers2)\n",
    "\n",
    "def nn_cl_fun():\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(neurons, input_dim=10, activation=activation))\n",
    "    if normalization > 0.5:\n",
    "        nn.add(BatchNormalization())\n",
    "    for i in range(layers1):\n",
    "        nn.add(Dense(neurons, activation=activation))\n",
    "    if dropout > 0.5:\n",
    "        nn.add(Dropout(dropout_rate, seed=123))\n",
    "    for i in range(layers2):\n",
    "        nn.add(Dense(neurons, activation=activation))\n",
    "    nn.add(Dense(1, activation='linear'))\n",
    "    nn.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[\"mean_squared_error\",\"mean_absolute_error\",\"mean_absolute_percentage_error\",\"mean_squared_logarithmic_error\"])\n",
    "    return nn\n",
    "es = EarlyStopping(monitor='val_loss', verbose=0, patience=20)\n",
    "nn = KerasRegressor(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round((10, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    hp_activation_2 = hp.Choice('activation_2', values=['relu', 'tanh'])\n",
    "    hp_activation_3 = hp.Choice('activation_3', values=['relu', 'tanh'])\n",
    "    hp_activation_4 = hp.Choice('activation_4', values=['relu', 'tanh'])\n",
    "    kwargs = {'hp_activation_0':hp_activation_2,'hp_activation_1':hp_activation_3,'hp_activation_2':hp_activation_4}\n",
    "    hp_weights=hp.Choice('weights', values=['glorot_uniform','he_normal','he_uniform','random_uniform','random_normal'])\n",
    "    for i,r in enumerate(range(hp.Int('num_layers', 2, 4))):\n",
    "        print(i,r)\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                            min_value=2,\n",
    "                                            max_value=10,\n",
    "                                            step=2),kernel_initializer=hp_weights,\n",
    "                               activation=kwargs[f'hp_activation_{i}']))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='mean_absolute_error',\n",
    "        metrics=['mean_absolute_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mean_absolute_error',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=1,\n",
    "    directory='project',\n",
    "    project_name='new5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 20s]\n",
      "val_mean_absolute_error: 0.0012792764464393258\n",
      "\n",
      "Best val_mean_absolute_error So Far: 0.0006915595731697977\n",
      "Total elapsed time: 00h 01m 34s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(dep_train, indep_train,\n",
    "             epochs=2,\n",
    "             validation_data=(dep_val, indep_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation_2': 'tanh',\n",
       " 'activation_3': 'relu',\n",
       " 'activation_4': 'tanh',\n",
       " 'weights': 'random_normal',\n",
       " 'num_layers': 3,\n",
       " 'units_0': 10,\n",
       " 'units_1': 8,\n",
       " 'learning_rate': 0.0001,\n",
       " 'units_2': 8}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = tuner.get_best_hyperparameters()\n",
    "\n",
    "best_params[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file_name_ids_inside form: {'activation': 'relu', 'weights': 'random_normal', 'num_layers': 3, 'units_0': 8, 'units_1': 6, 'learning_rate': 0.001, 'units_2': 2}\n"
     ]
    }
   ],
   "source": [
    "import yaml,sys\n",
    "with io.open('variable1.yaml', 'w', encoding='utf8') as outfile:\n",
    "    yaml.dump(best_params[0].values, outfile, default_flow_style=False, allow_unicode=True)\n",
    "print(\"file_name_ids_inside form:\",best_params[0].values,file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13bec6b3e23dd0921d86a1ec88a5a342f8b423eb92f661fc2c47e8b6a92d2fe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
